
    <!DOCTYPE html>
    <html>
    <head>
        <title>Analysis Report - 2003.08934v2.pdf</title>
        <style>
            body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 40px; background-color: #f4f4f9; color: #333; }
            .container { max-width: 900px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 4px 15px rgba(0,0,0,0.1); }
            h1 { border-bottom: 2px solid #ddd; padding-bottom: 10px; color: #2c3e50; }
            .dashboard { display: flex; justify-content: space-between; margin-bottom: 30px; background: #ecf0f1; padding: 20px; border-radius: 8px; }
            .metric { text-align: center; }
            .metric-val { font-size: 24px; font-weight: bold; display: block; }
            .metric-label { font-size: 14px; color: #7f8c8d; text-transform: uppercase; }
            
            /* Highlighting Classes */
            .ai-generated { color: #d63031; font-weight: 500; } /* Red Text */
            .plagiarism-highlight { background-color: #ffeaa7; color: black; border-bottom: 2px solid #fdcb6e; cursor: help; } /* Yellow BG */
            
            .legend { margin-bottom: 20px; padding: 10px; border: 1px solid #ddd; border-radius: 5px; font-size: 14px; background: #fff; }
            .dot { height: 10px; width: 10px; display: inline-block; border-radius: 50%; margin-right: 5px; }
            
            .document-content { line-height: 1.6; font-size: 16px; white-space: pre-wrap; }
            .paragraph { margin-bottom: 15px; position: relative; }
            .p-meta { font-size: 10px; color: #bdc3c7; position: absolute; left: -30px; top: 2px; }
            
            /* Tooltip for plagiarism source */
            .tooltip { display: none; position: absolute; background: #333; color: #fff; padding: 5px; font-size: 12px; border-radius: 4px; z-index: 100; max-width: 300px; }
            .plagiarism-highlight:hover + .tooltip { display: block; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>üìÑ Document Analysis Report</h1>
            <p><strong>File:</strong> C:\Users\athre\Downloads\2003.08934v2.pdf<br><strong>Date:</strong> 2025-12-02 18:11:07</p>
            
            <div class="dashboard">
                <div class="metric">
                    <span class="metric-val" style="color: #27ae60">16.9%</span>
                    <span class="metric-label">AI Score</span>
                </div>
                <div class="metric">
                    <span class="metric-val" style="color: #27ae60">0.0%</span>
                    <span class="metric-label">Plagiarism</span>
                </div>
                <div class="metric">
                    <span class="metric-val" style="color: #27ae60">0.0%</span>
                    <span class="metric-label">Citation Issues</span>
                </div>
            </div>

            <div class="legend">
                <strong>Legend:</strong>&nbsp;&nbsp; 
                <span class="dot" style="background: #d63031;"></span> <span style="color: #d63031">Red Text</span> = AI Generated &nbsp;|&nbsp; 
                <span class="dot" style="background: #ffeaa7; border: 1px solid #fdcb6e;"></span> <span style="background: #ffeaa7">Yellow Highlight</span> = Plagiarized
            </div>

            <div class="document-content">
    <div class="paragraph"><span class="p-meta">#1</span>0 2 0 2</div><div class="paragraph"><span class="p-meta">#2</span>g u A 3</div><div class="paragraph"><span class="p-meta">#3</span>]</div><div class="paragraph"><span class="p-meta">#4</span>V C . s c [</div><div class="paragraph"><span class="p-meta">#5</span>2 v 4 3 9 8 0 . 3 0 0 2 : v i X r a</div><div class="paragraph"><span class="p-meta">#6</span>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#7</span>Ben Mildenhall1(cid:63)</div><div class="paragraph"><span class="p-meta">#8</span>Pratul P. Srinivasan1(cid:63) Matthew Tancik1(cid:63)</div><div class="paragraph"><span class="p-meta">#9</span>Jonathan T. Barron2 Ravi Ramamoorthi3 Ren Ng1</div><div class="paragraph"><span class="p-meta">#10</span>1UC Berkeley</div><div class="paragraph"><span class="p-meta">#11</span>2Google Research</div><div class="paragraph"><span class="p-meta">#12</span>3UC San Diego</div><div class="paragraph ai-generated"><span class="p-meta">#13</span>Abstract. We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an under- lying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non- convolutional) deep network, whose input is a single continuous 5D coor- dinate (spatial location (x, y, z) and viewing direction (Œ∏, œÜ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally diÔ¨Äerentiable, the only input required to optimize our repre- sentation is a set of images with known camera poses. We describe how to eÔ¨Äectively optimize neural radiance Ô¨Åelds to render photorealistic novel views of scenes with complicated geometry and appearance, and demon- strate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.</div><div class="paragraph"><span class="p-meta">#14</span>Keywords: scene representation, view synthesis, image-based render- ing, volume rendering, 3D deep learning</div><div class="paragraph"><span class="p-meta">#15</span>1 Introduction</div><div class="paragraph"><span class="p-meta">#16</span>In this work, we address the long-standing problem of view synthesis in a new way by directly optimizing parameters of a continuous 5D scene representation to minimize the error of rendering a set of captured images.</div><div class="paragraph ai-generated"><span class="p-meta">#17</span>We represent a static scene as a continuous 5D function that outputs the radiance emitted in each direction (Œ∏, œÜ) at each point (x, y, z) in space, and a density at each point which acts like a diÔ¨Äerential opacity controlling how much radiance is accumulated by a ray passing through (x, y, z). Our method optimizes a deep fully-connected neural network without any convolutional layers (often referred to as a multilayer perceptron or MLP) to represent this function by regressing from a single 5D coordinate (x, y, z, Œ∏, œÜ) to a single volume density and view-dependent RGB color. To render this neural radiance Ô¨Åeld (NeRF)</div><div class="paragraph"><span class="p-meta">#18</span>(cid:63) Authors contributed equally to this work.</div><div class="paragraph"><span class="p-meta">#19</span>2</div><div class="paragraph"><span class="p-meta">#20</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph"><span class="p-meta">#21</span>Fig. 1: We present a method that optimizes a continuous 5D neural radiance Ô¨Åeld representation (volume density and view-dependent color at any continuous location) of a scene from a set of input images. We use techniques from volume rendering to accumulate samples of this scene representation along rays to render the scene from any viewpoint. Here, we visualize the set of 100 input views of the synthetic Drums scene randomly captured on a surrounding hemisphere, and we show two novel views rendered from our optimized NeRF representation.</div><div class="paragraph ai-generated"><span class="p-meta">#22</span>from a particular viewpoint we: 1) march camera rays through the scene to generate a sampled set of 3D points, 2) use those points and their corresponding 2D viewing directions as input to the neural network to produce an output set of colors and densities, and 3) use classical volume rendering techniques to accumulate those colors and densities into a 2D image. Because this process is naturally diÔ¨Äerentiable, we can use gradient descent to optimize this model by minimizing the error between each observed image and the corresponding views rendered from our representation. Minimizing this error across multiple views encourages the network to predict a coherent model of the scene by assigning high volume densities and accurate colors to the locations that contain the true underlying scene content. Figure 2 visualizes this overall pipeline.</div><div class="paragraph ai-generated"><span class="p-meta">#23</span>We Ô¨Ånd that the basic implementation of optimizing a neural radiance Ô¨Åeld representation for a complex scene does not converge to a suÔ¨Éciently high- resolution representation and is ineÔ¨Écient in the required number of samples per camera ray. We address these issues by transforming input 5D coordinates with a positional encoding that enables the MLP to represent higher frequency func- tions, and we propose a hierarchical sampling procedure to reduce the number of queries required to adequately sample this high-frequency scene representation. Our approach inherits the beneÔ¨Åts of volumetric representations: both can represent complex real-world geometry and appearance and are well suited for gradient-based optimization using projected images. Crucially, our method over- comes the prohibitive storage costs of discretized voxel grids when modeling complex scenes at high-resolutions. In summary, our technical contributions are: ‚Äì An approach for representing continuous scenes with complex geometry and materials as 5D neural radiance Ô¨Åelds, parameterized as basic MLP networks. ‚Äì A diÔ¨Äerentiable rendering procedure based on classical volume rendering tech- niques, which we use to optimize these representations from standard RGB images. This includes a hierarchical sampling strategy to allocate the MLP‚Äôs capacity towards space with visible scene content.</div><div class="paragraph"><span class="p-meta">#24</span>Input ImagesOptimize NeRFRender new viewsNeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#25</span>3</div><div class="paragraph"><span class="p-meta">#26</span>‚Äì A positional encoding to map each input 5D coordinate into a higher dimen- sional space, which enables us to successfully optimize neural radiance Ô¨Åelds to represent high-frequency scene content.</div><div class="paragraph ai-generated"><span class="p-meta">#27</span>We demonstrate that our resulting neural radiance Ô¨Åeld method quantitatively and qualitatively outperforms state-of-the-art view synthesis methods, including works that Ô¨Åt neural 3D representations to scenes as well as works that train deep convolutional networks to predict sampled volumetric representations. As far as we know, this paper presents the Ô¨Årst continuous neural scene representation that is able to render high-resolution photorealistic novel views of real objects and scenes from RGB images captured in natural settings.</div><div class="paragraph"><span class="p-meta">#28</span>2 Related Work</div><div class="paragraph ai-generated"><span class="p-meta">#29</span>A promising recent direction in computer vision is encoding objects and scenes in the weights of an MLP that directly maps from a 3D spatial location to an implicit representation of the shape, such as the signed distance [6] at that location. However, these methods have so far been unable to reproduce realistic scenes with complex geometry with the same Ô¨Ådelity as techniques that represent scenes using discrete representations such as triangle meshes or voxel grids. In this section, we review these two lines of work and contrast them with our approach, which enhances the capabilities of neural scene representations to produce state-of-the-art results for rendering complex realistic scenes.</div><div class="paragraph"><span class="p-meta">#30</span>A similar approach of using MLPs to map from low-dimensional coordinates to colors has also been used for representing other graphics functions such as im- ages [44], textured materials [12,31,36,37], and indirect illumination values [38].</div><div class="paragraph ai-generated"><span class="p-meta">#31</span>Neural 3D shape representations Recent work has investigated the im- plicit representation of continuous 3D shapes as level sets by optimizing deep networks that map xyz coordinates to signed distance functions [15,32] or occu- pancy Ô¨Åelds [11,27]. However, these models are limited by their requirement of access to ground truth 3D geometry, typically obtained from synthetic 3D shape datasets such as ShapeNet [3]. Subsequent work has relaxed this requirement of ground truth 3D shapes by formulating diÔ¨Äerentiable rendering functions that allow neural implicit shape representations to be optimized using only 2D im- ages. Niemeyer et al. [29] represent surfaces as 3D occupancy Ô¨Åelds and use a numerical method to Ô¨Ånd the surface intersection for each ray, then calculate an exact derivative using implicit diÔ¨Äerentiation. Each ray intersection location is provided as the input to a neural 3D texture Ô¨Åeld that predicts a diÔ¨Äuse color for that point. Sitzmann et al. [42] use a less direct neural 3D representation that simply outputs a feature vector and RGB color at each continuous 3D coordinate, and propose a diÔ¨Äerentiable rendering function consisting of a recurrent neural network that marches along each ray to decide where the surface is located.</div><div class="paragraph"><span class="p-meta">#32</span>Though these techniques can potentially represent complicated and high- resolution geometry, they have so far been limited to simple shapes with low geometric complexity, resulting in oversmoothed renderings. We show that an al- ternate strategy of optimizing networks to encode 5D radiance Ô¨Åelds (3D volumes</div><div class="paragraph"><span class="p-meta">#33</span>4</div><div class="paragraph"><span class="p-meta">#34</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph"><span class="p-meta">#35</span>with 2D view-dependent appearance) can represent higher-resolution geometry and appearance to render photorealistic novel views of complex scenes.</div><div class="paragraph"><span class="p-meta">#36</span>View synthesis and image-based rendering Given a dense sampling of views, photorealistic novel views can be reconstructed by simple light Ô¨Åeld sam- ple interpolation techniques [21,5,7]. For novel view synthesis with sparser view sampling, the computer vision and graphics communities have made signiÔ¨Åcant progress by predicting traditional geometry and appearance representations from observed images. One popular class of approaches uses mesh-based representa- tions of scenes with either diÔ¨Äuse [48] or view-dependent [2,8,49] appearance. DiÔ¨Äerentiable rasterizers [4,10,23,25] or pathtracers [22,30] can directly optimize mesh representations to reproduce a set of input images using gradient descent. However, gradient-based mesh optimization based on image reprojection is often diÔ¨Écult, likely because of local minima or poor conditioning of the loss land- scape. Furthermore, this strategy requires a template mesh with Ô¨Åxed topology to be provided as an initialization before optimization [22], which is typically unavailable for unconstrained real-world scenes.</div><div class="paragraph ai-generated"><span class="p-meta">#37</span>Another class of methods use volumetric representations to address the task of high-quality photorealistic view synthesis from a set of input RGB images. Volumetric approaches are able to realistically represent complex shapes and materials, are well-suited for gradient-based optimization, and tend to produce less visually distracting artifacts than mesh-based methods. Early volumetric approaches used observed images to directly color voxel grids [19,40,45]. More recently, several methods [9,13,17,28,33,43,46,52] have used large datasets of mul- tiple scenes to train deep networks that predict a sampled volumetric represen- tation from a set of input images, and then use either alpha-compositing [34] or learned compositing along rays to render novel views at test time. Other works have optimized a combination of convolutional networks (CNNs) and sampled voxel grids for each speciÔ¨Åc scene, such that the CNN can compensate for dis- cretization artifacts from low resolution voxel grids [41] or allow the predicted voxel grids to vary based on input time or animation controls [24]. While these volumetric techniques have achieved impressive results for novel view synthe- sis, their ability to scale to higher resolution imagery is fundamentally limited by poor time and space complexity due to their discrete sampling ‚Äî rendering higher resolution images requires a Ô¨Åner sampling of 3D space. We circumvent this problem by instead encoding a continuous volume within the parameters of a deep fully-connected neural network, which not only produces signiÔ¨Åcantly higher quality renderings than prior volumetric approaches, but also requires just a fraction of the storage cost of those sampled volumetric representations.</div><div class="paragraph"><span class="p-meta">#38</span>3 Neural Radiance Field Scene Representation</div><div class="paragraph"><span class="p-meta">#39</span>We represent a continuous scene as a 5D vector-valued function whose input is a 3D location x = (x, y, z) and 2D viewing direction (Œ∏, œÜ), and whose output is an emitted color c = (r, g, b) and volume density œÉ. In practice, we express</div><div class="paragraph"><span class="p-meta">#40</span>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#41</span>5</div><div class="paragraph"><span class="p-meta">#42</span>Fig. 2: An overview of our neural radiance Ô¨Åeld scene representation and diÔ¨Äer- entiable rendering procedure. We synthesize images by sampling 5D coordinates (location and viewing direction) along camera rays (a), feeding those locations into an MLP to produce a color and volume density (b), and using volume ren- dering techniques to composite these values into an image (c). This rendering function is diÔ¨Äerentiable, so we can optimize our scene representation by mini- mizing the residual between synthesized and ground truth observed images (d).</div><div class="paragraph"><span class="p-meta">#43</span>direction as a 3D Cartesian unit vector d. We approximate this continuous 5D scene representation with an MLP network FŒò : (x, d) ‚Üí (c, œÉ) and optimize its weights Œò to map from each input 5D coordinate to its corresponding volume density and directional emitted color.</div><div class="paragraph ai-generated"><span class="p-meta">#44</span>We encourage the representation to be multiview consistent by restricting the network to predict the volume density œÉ as a function of only the location x, while allowing the RGB color c to be predicted as a function of both location and viewing direction. To accomplish this, the MLP FŒò Ô¨Årst processes the input 3D coordinate x with 8 fully-connected layers (using ReLU activations and 256 channels per layer), and outputs œÉ and a 256-dimensional feature vector. This feature vector is then concatenated with the camera ray‚Äôs viewing direction and passed to one additional fully-connected layer (using a ReLU activation and 128 channels) that output the view-dependent RGB color.</div><div class="paragraph"><span class="p-meta">#45</span>See Fig. 3 for an example of how our method uses the input viewing direction to represent non-Lambertian eÔ¨Äects. As shown in Fig. 4, a model trained without view dependence (only x as input) has diÔ¨Éculty representing specularities.</div><div class="paragraph"><span class="p-meta">#46</span>4 Volume Rendering with Radiance Fields</div><div class="paragraph"><span class="p-meta">#47</span>Our 5D neural radiance Ô¨Åeld represents a scene as the volume density and di- rectional emitted radiance at any point in space. We render the color of any ray passing through the scene using principles from classical volume rendering [16]. The volume density œÉ(x) can be interpreted as the diÔ¨Äerential probability of a ray terminating at an inÔ¨Ånitesimal particle at location x. The expected color C(r) of camera ray r(t) = o + td with near and far bounds tn and tf is:</div><div class="paragraph"><span class="p-meta">#48</span>C(r) =</div><div class="paragraph"><span class="p-meta">#49</span>(cid:90) tf</div><div class="paragraph"><span class="p-meta">#50</span>tn</div><div class="paragraph"><span class="p-meta">#51</span>T (t)œÉ(r(t))c(r(t), d)dt , where T (t) = exp</div><div class="paragraph"><span class="p-meta">#52</span>‚àí</div><div class="paragraph"><span class="p-meta">#53</span>(cid:18)</div><div class="paragraph"><span class="p-meta">#54</span>(cid:19)</div><div class="paragraph"><span class="p-meta">#55</span>œÉ(r(s))ds</div><div class="paragraph"><span class="p-meta">#56</span>. (1)</div><div class="paragraph"><span class="p-meta">#57</span>(cid:90) t</div><div class="paragraph"><span class="p-meta">#58</span>tn</div><div class="paragraph"><span class="p-meta">#59</span>(x,y,z,Œ∏,œï)FŒò(RGBœÉ)5D InputPosition + DirectionOutputColor + DensityVolume RenderingRay 1œÉœÉRenderingLossg.t.g.t.2222Ray 2Ray 1Ray Distance(b)(a)(c)(d)Ray 26</div><div class="paragraph"><span class="p-meta">#60</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph ai-generated"><span class="p-meta">#61</span>Fig. 3: A visualization of view-dependent emitted radiance. Our neural radiance Ô¨Åeld representation outputs RGB color as a 5D function of both spatial position x and viewing direction d. Here, we visualize example directional color distri- butions for two spatial locations in our neural representation of the Ship scene. In (a) and (b), we show the appearance of two Ô¨Åxed 3D points from two dif- ferent camera positions: one on the side of the ship (orange insets) and one on the surface of the water (blue insets). Our method predicts the changing spec- ular appearance of these two 3D points, and in (c) we show how this behavior generalizes continuously across the whole hemisphere of viewing directions.</div><div class="paragraph"><span class="p-meta">#62</span>The function T (t) denotes the accumulated transmittance along the ray from tn to t, i.e., the probability that the ray travels from tn to t without hitting any other particle. Rendering a view from our continuous neural radiance Ô¨Åeld requires estimating this integral C(r) for a camera ray traced through each pixel of the desired virtual camera.</div><div class="paragraph"><span class="p-meta">#63</span>We numerically estimate this continuous integral using quadrature. Deter- ministic quadrature, which is typically used for rendering discretized voxel grids, would eÔ¨Äectively limit our representation‚Äôs resolution because the MLP would only be queried at a Ô¨Åxed discrete set of locations. Instead, we use a stratiÔ¨Åed sampling approach where we partition [tn, tf ] into N evenly-spaced bins and then draw one sample uniformly at random from within each bin:</div><div class="paragraph"><span class="p-meta">#64</span>(cid:20)</div><div class="paragraph"><span class="p-meta">#65</span>ti ‚àº U</div><div class="paragraph"><span class="p-meta">#66</span>tn +</div><div class="paragraph"><span class="p-meta">#67</span>i ‚àí 1 N</div><div class="paragraph"><span class="p-meta">#68</span>(tf ‚àí tn), tn +</div><div class="paragraph"><span class="p-meta">#69</span>(cid:21) (tf ‚àí tn)</div><div class="paragraph"><span class="p-meta">#70</span>.</div><div class="paragraph"><span class="p-meta">#71</span>i N</div><div class="paragraph"><span class="p-meta">#72</span>(2)</div><div class="paragraph"><span class="p-meta">#73</span>Although we use a discrete set of samples to estimate the integral, stratiÔ¨Åed sampling enables us to represent a continuous scene representation because it results in the MLP being evaluated at continuous positions over the course of optimization. We use these samples to estimate C(r) with the quadrature rule discussed in the volume rendering review by Max [26]:</div><div class="paragraph"><span class="p-meta">#74</span>ÀÜC(r) =</div><div class="paragraph"><span class="p-meta">#75</span>N (cid:88)</div><div class="paragraph"><span class="p-meta">#76</span>i=1</div><div class="paragraph"><span class="p-meta">#77</span>Ti(1 ‚àí exp(‚àíœÉiŒ¥i))ci , where Ti = exp</div><div class="paragraph"><span class="p-meta">#78</span>Ô£≠‚àí</div><div class="paragraph"><span class="p-meta">#79</span>Ô£´</div><div class="paragraph"><span class="p-meta">#80</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#81</span>œÉjŒ¥j</div><div class="paragraph"><span class="p-meta">#82</span>Ô£∏ ,</div><div class="paragraph"><span class="p-meta">#83</span>(3)</div><div class="paragraph"><span class="p-meta">#84</span>i‚àí1 (cid:88)</div><div class="paragraph"><span class="p-meta">#85</span>j=1</div><div class="paragraph"><span class="p-meta">#86</span>where Œ¥i = ti+1 ‚àí ti is the distance between adjacent samples. This function for calculating ÀÜC(r) from the set of (ci, œÉi) values is trivially diÔ¨Äerentiable and reduces to traditional alpha compositing with alpha values Œ±i = 1 ‚àí exp(‚àíœÉiŒ¥i).</div><div class="paragraph"><span class="p-meta">#87</span>(a) View 1(b) View 2(c) Radiance DistributionsNeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#88</span>7</div><div class="paragraph"><span class="p-meta">#89</span>Ground Truth</div><div class="paragraph"><span class="p-meta">#90</span>Complete Model</div><div class="paragraph"><span class="p-meta">#91</span>No View Dependence No Positional Encoding</div><div class="paragraph"><span class="p-meta">#92</span>Fig. 4: Here we visualize how our full model beneÔ¨Åts from representing view- dependent emitted radiance and from passing our input coordinates through a high-frequency positional encoding. Removing view dependence prevents the model from recreating the specular reÔ¨Çection on the bulldozer tread. Removing the positional encoding drastically decreases the model‚Äôs ability to represent high frequency geometry and texture, resulting in an oversmoothed appearance.</div><div class="paragraph"><span class="p-meta">#93</span>5 Optimizing a Neural Radiance Field</div><div class="paragraph ai-generated"><span class="p-meta">#94</span>In the previous section we have described the core components necessary for modeling a scene as a neural radiance Ô¨Åeld and rendering novel views from this representation. However, we observe that these components are not suÔ¨Écient for achieving state-of-the-art quality, as demonstrated in Section 6.4). We introduce two improvements to enable representing high-resolution complex scenes. The Ô¨Årst is a positional encoding of the input coordinates that assists the MLP in representing high-frequency functions, and the second is a hierarchical sampling procedure that allows us to eÔ¨Éciently sample this high-frequency representation.</div><div class="paragraph"><span class="p-meta">#95</span>5.1 Positional encoding</div><div class="paragraph ai-generated"><span class="p-meta">#96</span>Despite the fact that neural networks are universal function approximators [14], we found that having the network FŒò directly operate on xyzŒ∏œÜ input coordi- nates results in renderings that perform poorly at representing high-frequency variation in color and geometry. This is consistent with recent work by Rahaman et al. [35], which shows that deep networks are biased towards learning lower fre- quency functions. They additionally show that mapping the inputs to a higher dimensional space using high frequency functions before passing them to the network enables better Ô¨Åtting of data that contains high frequency variation.</div><div class="paragraph"><span class="p-meta">#97</span>We leverage these Ô¨Åndings in the context of neural scene representations, and show that reformulating FŒò as a composition of two functions FŒò = F (cid:48) Œò ‚ó¶ Œ≥, one learned and one not, signiÔ¨Åcantly improves performance (see Fig. 4 and Table 2). Here Œ≥ is a mapping from R into a higher dimensional space R2L, and F (cid:48) Œò is still simply a regular MLP. Formally, the encoding function we use is:</div><div class="paragraph"><span class="p-meta">#98</span>Œ≥(p) = (cid:0) sin(cid:0)20œÄp(cid:1), cos(cid:0)20œÄp(cid:1), ¬∑ ¬∑ ¬∑ , sin(cid:0)2L‚àí1œÄp(cid:1), cos(cid:0)2L‚àí1œÄp(cid:1) (cid:1) .</div><div class="paragraph"><span class="p-meta">#99</span>(4)</div><div class="paragraph"><span class="p-meta">#100</span>This function Œ≥(¬∑) is applied separately to each of the three coordinate values in x (which are normalized to lie in [‚àí1, 1]) and to the three components of the</div><div class="paragraph"><span class="p-meta">#101</span>8</div><div class="paragraph"><span class="p-meta">#102</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph ai-generated"><span class="p-meta">#103</span>Cartesian viewing direction unit vector d (which by construction lie in [‚àí1, 1]). In our experiments, we set L = 10 for Œ≥(x) and L = 4 for Œ≥(d).</div><div class="paragraph ai-generated"><span class="p-meta">#104</span>A similar mapping is used in the popular Transformer architecture [47], where it is referred to as a positional encoding. However, Transformers use it for a diÔ¨Äerent goal of providing the discrete positions of tokens in a sequence as input to an architecture that does not contain any notion of order. In contrast, we use these functions to map continuous input coordinates into a higher dimensional space to enable our MLP to more easily approximate a higher frequency function. Concurrent work on a related problem of modeling 3D protein structure from projections [51] also utilizes a similar input coordinate mapping.</div><div class="paragraph"><span class="p-meta">#105</span>5.2 Hierarchical volume sampling</div><div class="paragraph"><span class="p-meta">#106</span>Our rendering strategy of densely evaluating the neural radiance Ô¨Åeld network at N query points along each camera ray is ineÔ¨Écient: free space and occluded regions that do not contribute to the rendered image are still sampled repeat- edly. We draw inspiration from early work in volume rendering [20] and propose a hierarchical representation that increases rendering eÔ¨Éciency by allocating samples proportionally to their expected eÔ¨Äect on the Ô¨Ånal rendering.</div><div class="paragraph ai-generated"><span class="p-meta">#107</span>Instead of just using a single network to represent the scene, we simultane- ously optimize two networks: one ‚Äúcoarse‚Äù and one ‚ÄúÔ¨Åne‚Äù. We Ô¨Årst sample a set of Nc locations using stratiÔ¨Åed sampling, and evaluate the ‚Äúcoarse‚Äù network at these locations as described in Eqns. 2 and 3. Given the output of this ‚Äúcoarse‚Äù network, we then produce a more informed sampling of points along each ray where samples are biased towards the relevant parts of the volume. To do this, we Ô¨Årst rewrite the alpha composited color from the coarse network ÀÜCc(r) in Eqn. 3 as a weighted sum of all sampled colors ci along the ray:</div><div class="paragraph"><span class="p-meta">#108</span>ÀÜCc(r) =</div><div class="paragraph"><span class="p-meta">#109</span>Nc(cid:88)</div><div class="paragraph"><span class="p-meta">#110</span>i=1</div><div class="paragraph"><span class="p-meta">#111</span>wici ,</div><div class="paragraph"><span class="p-meta">#112</span>wi = Ti(1 ‚àí exp(‚àíœÉiŒ¥i)) .</div><div class="paragraph"><span class="p-meta">#113</span>(5)</div><div class="paragraph"><span class="p-meta">#114</span>Normalizing these weights as ÀÜwi = wi/(cid:80)Nc j=1 wj produces a piecewise-constant PDF along the ray. We sample a second set of Nf locations from this distribution using inverse transform sampling, evaluate our ‚ÄúÔ¨Åne‚Äù network at the union of the Ô¨Årst and second set of samples, and compute the Ô¨Ånal rendered color of the ray ÀÜCf (r) using Eqn. 3 but using all Nc +Nf samples. This procedure allocates more samples to regions we expect to contain visible content. This addresses a similar goal as importance sampling, but we use the sampled values as a nonuniform discretization of the whole integration domain rather than treating each sample as an independent probabilistic estimate of the entire integral.</div><div class="paragraph"><span class="p-meta">#115</span>5.3</div><div class="paragraph"><span class="p-meta">#116</span>Implementation details</div><div class="paragraph"><span class="p-meta">#117</span>We optimize a separate neural continuous volume representation network for each scene. This requires only a dataset of captured RGB images of the scene,</div><div class="paragraph"><span class="p-meta">#118</span>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#119</span>9</div><div class="paragraph ai-generated"><span class="p-meta">#120</span>the corresponding camera poses and intrinsic parameters, and scene bounds (we use ground truth camera poses, intrinsics, and bounds for synthetic data, and use the COLMAP structure-from-motion package [39] to estimate these parameters for real data). At each optimization iteration, we randomly sample a batch of camera rays from the set of all pixels in the dataset, and then follow the hierarchical sampling described in Sec. 5.2 to query Nc samples from the coarse network and Nc + Nf samples from the Ô¨Åne network. We then use the volume rendering procedure described in Sec. 4 to render the color of each ray from both sets of samples. Our loss is simply the total squared error between the rendered and true pixel colors for both the coarse and Ô¨Åne renderings:</div><div class="paragraph"><span class="p-meta">#121</span>L =</div><div class="paragraph"><span class="p-meta">#122</span>(cid:88)</div><div class="paragraph"><span class="p-meta">#123</span>r‚ààR</div><div class="paragraph"><span class="p-meta">#124</span>(cid:20)(cid:13) (cid:13) 2 ÀÜCc(r) ‚àí C(r) (cid:13) (cid:13) (cid:13) (cid:13) 2</div><div class="paragraph"><span class="p-meta">#125</span>+</div><div class="paragraph"><span class="p-meta">#126</span>(cid:13) ÀÜCf (r) ‚àí C(r) (cid:13) (cid:13)</div><div class="paragraph"><span class="p-meta">#127</span>(cid:13) 2 (cid:13) (cid:13) 2</div><div class="paragraph"><span class="p-meta">#128</span>(cid:21)</div><div class="paragraph"><span class="p-meta">#129</span>(6)</div><div class="paragraph ai-generated"><span class="p-meta">#130</span>where R is the set of rays in each batch, and C(r), ÀÜCc(r), and ÀÜCf (r) are the ground truth, coarse volume predicted, and Ô¨Åne volume predicted RGB colors for ray r respectively. Note that even though the Ô¨Ånal rendering comes from ÀÜCf (r), we also minimize the loss of ÀÜCc(r) so that the weight distribution from the coarse network can be used to allocate samples in the Ô¨Åne network.</div><div class="paragraph ai-generated"><span class="p-meta">#131</span>In our experiments, we use a batch size of 4096 rays, each sampled at Nc = 64 coordinates in the coarse volume and Nf = 128 additional coordinates in the Ô¨Åne volume. We use the Adam optimizer [18] with a learning rate that begins at 5 √ó 10‚àí4 and decays exponentially to 5 √ó 10‚àí5 over the course of optimization (other Adam hyperparameters are left at default values of Œ≤1 = 0.9, Œ≤2 = 0.999, and (cid:15) = 10‚àí7). The optimization for a single scene typically take around 100‚Äì 300k iterations to converge on a single NVIDIA V100 GPU (about 1‚Äì2 days).</div><div class="paragraph"><span class="p-meta">#132</span>6 Results</div><div class="paragraph"><span class="p-meta">#133</span>We quantitatively (Tables 1) and qualitatively (Figs. 8 and 6) show that our method outperforms prior work, and provide extensive ablation studies to vali- date our design choices (Table 2). We urge the reader to view our supplementary video to better appreciate our method‚Äôs signiÔ¨Åcant improvement over baseline methods when rendering smooth paths of novel views.</div><div class="paragraph"><span class="p-meta">#134</span>6.1 Datasets</div><div class="paragraph ai-generated"><span class="p-meta">#135</span>Synthetic renderings of objects We Ô¨Årst show experimental results on two datasets of synthetic renderings of objects (Table 1, ‚ÄúDiÔ¨Äuse Synthetic 360‚ó¶‚Äù and ‚ÄúRealistic Synthetic 360‚ó¶‚Äù). The DeepVoxels [41] dataset contains four Lamber- tian objects with simple geometry. Each object is rendered at 512 √ó 512 pixels from viewpoints sampled on the upper hemisphere (479 as input and 1000 for testing). We additionally generate our own dataset containing pathtraced images of eight objects that exhibit complicated geometry and realistic non-Lambertian materials. Six are rendered from viewpoints sampled on the upper hemisphere, and two are rendered from viewpoints sampled on a full sphere. We render 100 views of each scene as input and 200 for testing, all at 800 √ó 800 pixels.</div><div class="paragraph"><span class="p-meta">#136</span>10</div><div class="paragraph"><span class="p-meta">#137</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph"><span class="p-meta">#138</span>Method SRN [42] NV [24] LLFF [28] Ours</div><div class="paragraph"><span class="p-meta">#139</span>DiÔ¨Äuse Synthetic 360‚ó¶ [41] Realistic Synthetic 360‚ó¶ Real Forward-Facing [28] PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì 0.846 33.20 0.893 29.62 0.911 34.38 0.947 40.15</div><div class="paragraph"><span class="p-meta">#140</span>0.073 0.099 0.048 0.023</div><div class="paragraph"><span class="p-meta">#141</span>0.170 0.160 0.114 0.081</div><div class="paragraph"><span class="p-meta">#142</span>0.378 - 0.212 0.250</div><div class="paragraph"><span class="p-meta">#143</span>22.84 - 24.13 26.50</div><div class="paragraph"><span class="p-meta">#144</span>22.26 26.05 24.88 31.01</div><div class="paragraph"><span class="p-meta">#145</span>0.668 - 0.798 0.811</div><div class="paragraph"><span class="p-meta">#146</span>0.963 0.929 0.985 0.991</div><div class="paragraph"><span class="p-meta">#147</span>Table 1: Our method quantitatively outperforms prior work on datasets of both synthetic and real images. We report PSNR/SSIM (higher is better) and LPIPS [50] (lower is better). The DeepVoxels [41] dataset consists of 4 diÔ¨Äuse ob- jects with simple geometry. Our realistic synthetic dataset consists of pathtraced renderings of 8 geometrically complex objects with complex non-Lambertian ma- terials. The real dataset consists of handheld forward-facing captures of 8 real- world scenes (NV cannot be evaluated on this data because it only reconstructs objects inside a bounded volume). Though LLFF achieves slightly better LPIPS, we urge readers to view our supplementary video where our method achieves better multiview consistency and produces fewer artifacts than all baselines.</div><div class="paragraph"><span class="p-meta">#148</span>Real images of complex scenes We show results on complex real-world scenes captured with roughly forward-facing images (Table 1, ‚ÄúReal Forward- Facing‚Äù). This dataset consists of 8 scenes captured with a handheld cellphone (5 taken from the LLFF paper and 3 that we capture), captured with 20 to 62 images, and hold out 1/8 of these for the test set. All images are 1008√ó756 pixels.</div><div class="paragraph"><span class="p-meta">#149</span>6.2 Comparisons</div><div class="paragraph ai-generated"><span class="p-meta">#150</span>To evaluate our model we compare against current top-performing techniques for view synthesis, detailed below. All methods use the same set of input views to train a separate network for each scene except Local Light Field Fusion [28], which trains a single 3D convolutional network on a large dataset, then uses the same trained network to process input images of new scenes at test time.</div><div class="paragraph"><span class="p-meta">#151</span>Neural Volumes (NV) [24] synthesizes novel views of objects that lie en- tirely within a bounded volume in front of a distinct background (which must be separately captured without the object of interest). It optimizes a deep 3D convolutional network to predict a discretized RGBŒ± voxel grid with 1283 sam- ples as well as a 3D warp grid with 323 samples. The algorithm renders novel views by marching camera rays through the warped voxel grid.</div><div class="paragraph ai-generated"><span class="p-meta">#152</span>Scene Representation Networks (SRN) [42] represent a continuous scene as an opaque surface, implicitly deÔ¨Åned by a MLP that maps each (x, y, z) co- ordinate to a feature vector. They train a recurrent neural network to march along a ray through the scene representation by using the feature vector at any 3D coordinate to predict the next step size along the ray. The feature vector from the Ô¨Ånal step is decoded into a single color for that point on the surface. Note that SRN is a better-performing followup to DeepVoxels [41] by the same authors, which is why we do not include comparisons to DeepVoxels.</div><div class="paragraph"><span class="p-meta">#153</span>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#154</span>11</div><div class="paragraph"><span class="p-meta">#155</span>Ship</div><div class="paragraph"><span class="p-meta">#156</span>Lego</div><div class="paragraph"><span class="p-meta">#157</span>Microphone</div><div class="paragraph"><span class="p-meta">#158</span>Materials</div><div class="paragraph"><span class="p-meta">#159</span>Ground Truth NeRF (ours)</div><div class="paragraph"><span class="p-meta">#160</span>LLFF [28]</div><div class="paragraph"><span class="p-meta">#161</span>SRN [42]</div><div class="paragraph"><span class="p-meta">#162</span>NV [24]</div><div class="paragraph ai-generated"><span class="p-meta">#163</span>Fig. 5: Comparisons on test-set views for scenes from our new synthetic dataset generated with a physically-based renderer. Our method is able to recover Ô¨Åne details in both geometry and appearance, such as Ship‚Äôs rigging, Lego‚Äôs gear and treads, Microphone‚Äôs shiny stand and mesh grille, and Material ‚Äôs non- Lambertian reÔ¨Çectance. LLFF exhibits banding artifacts on the Microphone stand and Material ‚Äôs object edges and ghosting artifacts in Ship‚Äôs mast and inside the Lego object. SRN produces blurry and distorted renderings in every case. Neural Volumes cannot capture the details on the Microphone‚Äôs grille or Lego‚Äôs gears, and it completely fails to recover the geometry of Ship‚Äôs rigging.</div><div class="paragraph"><span class="p-meta">#164</span>12</div><div class="paragraph"><span class="p-meta">#165</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph"><span class="p-meta">#166</span>Fern</div><div class="paragraph"><span class="p-meta">#167</span>T-Rex</div><div class="paragraph"><span class="p-meta">#168</span>Orchid</div><div class="paragraph"><span class="p-meta">#169</span>Ground Truth</div><div class="paragraph"><span class="p-meta">#170</span>NeRF (ours)</div><div class="paragraph"><span class="p-meta">#171</span>LLFF [28]</div><div class="paragraph"><span class="p-meta">#172</span>SRN [42]</div><div class="paragraph ai-generated"><span class="p-meta">#173</span>Fig. 6: Comparisons on test-set views of real world scenes. LLFF is speciÔ¨Åcally designed for this use case (forward-facing captures of real scenes). Our method is able to represent Ô¨Åne geometry more consistently across rendered views than LLFF, as shown in Fern‚Äôs leaves and the skeleton ribs and railing in T-rex. Our method also correctly reconstructs partially occluded regions that LLFF struggles to render cleanly, such as the yellow shelves behind the leaves in the bottom Fern crop and green leaves in the background of the bottom Orchid crop. Blending between multiples renderings can also cause repeated edges in LLFF, as seen in the top Orchid crop. SRN captures the low-frequency geometry and color variation in each scene but is unable to reproduce any Ô¨Åne detail.</div><div class="paragraph"><span class="p-meta">#174</span>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#175</span>13</div><div class="paragraph"><span class="p-meta">#176</span>Local Light Field Fusion (LLFF) [28] LLFF is designed for producing pho- torealistic novel views for well-sampled forward facing scenes. It uses a trained 3D convolutional network to directly predict a discretized frustum-sampled RGBŒ± grid (multiplane image or MPI [52]) for each input view, then renders novel views by alpha compositing and blending nearby MPIs into the novel viewpoint.</div><div class="paragraph"><span class="p-meta">#177</span>6.3 Discussion</div><div class="paragraph"><span class="p-meta">#178</span>We thoroughly outperform both baselines that also optimize a separate network per scene (NV and SRN) in all scenarios. Furthermore, we produce qualitatively and quantitatively superior renderings compared to LLFF (across all except one metric) while using only their input images as our entire training set.</div><div class="paragraph"><span class="p-meta">#179</span>The SRN method produces heavily smoothed geometry and texture, and its representational power for view synthesis is limited by selecting only a single depth and color per camera ray. The NV baseline is able to capture reasonably detailed volumetric geometry and appearance, but its use of an underlying ex- plicit 1283 voxel grid prevents it from scaling to represent Ô¨Åne details at high resolutions. LLFF speciÔ¨Åcally provides a ‚Äúsampling guideline‚Äù to not exceed 64 pixels of disparity between input views, so it frequently fails to estimate cor- rect geometry in the synthetic datasets which contain up to 400-500 pixels of disparity between views. Additionally, LLFF blends between diÔ¨Äerent scene rep- resentations for rendering diÔ¨Äerent views, resulting in perceptually-distracting inconsistency as is apparent in our supplementary video.</div><div class="paragraph"><span class="p-meta">#180</span>The biggest practical tradeoÔ¨Äs between these methods are time versus space. All compared single scene methods take at least 12 hours to train per scene. In contrast, LLFF can process a small input dataset in under 10 minutes. However, LLFF produces a large 3D voxel grid for every input image, resulting in enor- mous storage requirements (over 15GB for one ‚ÄúRealistic Synthetic‚Äù scene). Our method requires only 5 MB for the network weights (a relative compression of 3000√ó compared to LLFF), which is even less memory than the input images alone for a single scene from any of our datasets.</div><div class="paragraph"><span class="p-meta">#181</span>6.4 Ablation studies</div><div class="paragraph ai-generated"><span class="p-meta">#182</span>We validate our algorithm‚Äôs design choices and parameters with an extensive ablation study in Table 2. We present results on our ‚ÄúRealistic Synthetic 360‚ó¶‚Äù scenes. Row 9 shows our complete model as a point of reference. Row 1 shows a minimalist version of our model without positional encoding (PE), view- dependence (VD), or hierarchical sampling (H). In rows 2‚Äì4 we remove these three components one at a time from the full model, observing that positional encoding (row 2) and view-dependence (row 3) provide the largest quantitative beneÔ¨Åt followed by hierarchical sampling (row 4). Rows 5‚Äì6 show how our per- formance decreases as the number of input images is reduced. Note that our method‚Äôs performance using only 25 input images still exceeds NV, SRN, and LLFF across all metrics when they are provided with 100 images (see supple- mentary material). In rows 7‚Äì8 we validate our choice of the maximum frequency</div><div class="paragraph"><span class="p-meta">#183</span>14</div><div class="paragraph"><span class="p-meta">#184</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph"><span class="p-meta">#185</span>1) No PE, VD, H 2) No Pos. Encoding 3) No View Dependence 4) No Hierarchical 5) Far Fewer Images 6) Fewer Images 7) Fewer Frequencies 8) More Frequencies 9) Complete Model</div><div class="paragraph"><span class="p-meta">#186</span>Input #Im. L ( Nc , Nf ) PSNR‚Üë SSIM‚Üë LPIPS‚Üì xyz xyzŒ∏œÜ xyz xyzŒ∏œÜ xyzŒ∏œÜ xyzŒ∏œÜ xyzŒ∏œÜ xyzŒ∏œÜ xyzŒ∏œÜ</div><div class="paragraph"><span class="p-meta">#187</span>(256, - ) (64, 128) (64, 128) (256, - ) (64, 128) (64, 128) (64, 128) (64, 128) (64, 128)</div><div class="paragraph"><span class="p-meta">#188</span>0.136 0.108 0.117 0.109 0.107 0.096 0.088 0.096 0.081</div><div class="paragraph"><span class="p-meta">#189</span>26.67 28.77 27.66 30.06 27.78 29.79 30.59 30.81 31.01</div><div class="paragraph"><span class="p-meta">#190</span>0.906 0.924 0.925 0.938 0.925 0.940 0.944 0.946 0.947</div><div class="paragraph"><span class="p-meta">#191</span>100 100 100 100 25 50 100 100 100</div><div class="paragraph"><span class="p-meta">#192</span>- - 10 10 10 10 5 15 10</div><div class="paragraph"><span class="p-meta">#193</span>Table 2: An ablation study of our model. Metrics are averaged over the 8 scenes from our realistic synthetic dataset. See Sec. 6.4 for detailed descriptions.</div><div class="paragraph"><span class="p-meta">#194</span>L used in our positional encoding for x (the maximum frequency used for d is scaled proportionally). Only using 5 frequencies reduces performance, but in- creasing the number of frequencies from 10 to 15 does not improve performance. We believe the beneÔ¨Åt of increasing L is limited once 2L exceeds the maximum frequency present in the sampled input images (roughly 1024 in our data).</div><div class="paragraph"><span class="p-meta">#195</span>7 Conclusion</div><div class="paragraph ai-generated"><span class="p-meta">#196</span>Our work directly addresses deÔ¨Åciencies of prior work that uses MLPs to repre- sent objects and scenes as continuous functions. We demonstrate that represent- ing scenes as 5D neural radiance Ô¨Åelds (an MLP that outputs volume density and view-dependent emitted radiance as a function of 3D location and 2D viewing direction) produces better renderings than the previously-dominant approach of training deep convolutional networks to output discretized voxel representations. Although we have proposed a hierarchical sampling strategy to make render- ing more sample-eÔ¨Écient (for both training and testing), there is still much more progress to be made in investigating techniques to eÔ¨Éciently optimize and ren- der neural radiance Ô¨Åelds. Another direction for future work is interpretability: sampled representations such as voxel grids and meshes admit reasoning about the expected quality of rendered views and failure modes, but it is unclear how to analyze these issues when we encode scenes in the weights of a deep neural network. We believe that this work makes progress towards a graphics pipeline based on real world imagery, where complex scenes could be composed of neural radiance Ô¨Åelds optimized from images of actual objects and scenes.</div><div class="paragraph ai-generated"><span class="p-meta">#197</span>Acknowledgements We thank Kevin Cao, Guowei Frank Yang, and Nithin Raghavan for comments and discussions. RR acknowledges funding from ONR grants N000141712687 and N000142012529 and the Ronald L. Graham Chair. BM is funded by a Hertz Foundation Fellowship, and MT is funded by an NSF Graduate Fellowship. Google provided a generous donation of cloud com- pute credits through the BAIR Commons program. We thank the following</div><div class="paragraph"><span class="p-meta">#198</span>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#199</span>15</div><div class="paragraph"><span class="p-meta">#200</span>Blend Swap users for the models used in our realistic synthetic dataset: gregzaal (ship), 1DInc (chair), bryanajones (drums), Herberhold (Ô¨Åcus), erickfree (hot- dog), Heinzelnisse (lego), elbrujodelatribu (materials), and up3d.de (mic).</div><div class="paragraph"><span class="p-meta">#201</span>References</div><div class="paragraph"><span class="p-meta">#202</span>1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man¬¥e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vi¬¥egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015) 2. Buehler, C., Bosse, M., McMillan, L., Gortler, S., Cohen, M.: Unstructured lumi-</div><div class="paragraph"><span class="p-meta">#203</span>graph rendering. In: SIGGRAPH (2001)</div><div class="paragraph"><span class="p-meta">#204</span>3. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., et al.: Shapenet: An information-rich 3d model repository. arXiv:1512.03012 (2015)</div><div class="paragraph"><span class="p-meta">#205</span>4. Chen, W., Gao, J., Ling, H., Smith, E.J., Lehtinen, J., Jacobson, A., Fidler, S.: Learning to predict 3D objects with an interpolation-based diÔ¨Äerentiable renderer. In: NeurIPS (2019)</div><div class="paragraph"><span class="p-meta">#206</span>5. Cohen, M., Gortler, S.J., Szeliski, R., Grzeszczuk, R., Szeliski, R.: The lumigraph.</div><div class="paragraph"><span class="p-meta">#207</span>In: SIGGRAPH (1996)</div><div class="paragraph"><span class="p-meta">#208</span>6. Curless, B., Levoy, M.: A volumetric method for building complex models from</div><div class="paragraph"><span class="p-meta">#209</span>range images. In: SIGGRAPH (1996)</div><div class="paragraph ai-generated"><span class="p-meta">#210</span>7. Davis, A., Levoy, M., Durand, F.: Unstructured light Ô¨Åelds. In: Eurographics (2012) 8. Debevec, P., Taylor, C.J., Malik, J.: Modeling and rendering architecture from pho- tographs: A hybrid geometry-and image-based approach. In: SIGGRAPH (1996) 9. Flynn, J., Broxton, M., Debevec, P., DuVall, M., FyÔ¨Äe, G., Overbeck, R., Snavely, N., Tucker, R.: DeepView: view synthesis with learned gradient descent. In: CVPR (2019)</div><div class="paragraph"><span class="p-meta">#211</span>10. Genova, K., Cole, F., Maschinot, A., Sarna, A., Vlasic, D., , Freeman, W.T.: Un-</div><div class="paragraph"><span class="p-meta">#212</span>supervised training for 3D morphable model regression. In: CVPR (2018)</div><div class="paragraph"><span class="p-meta">#213</span>11. Genova, K., Cole, F., Sud, A., Sarna, A., Funkhouser, T.: Local deep implicit</div><div class="paragraph"><span class="p-meta">#214</span>functions for 3d shape. In: CVPR (2020)</div><div class="paragraph"><span class="p-meta">#215</span>12. Henzler, P., Mitra, N.J., Ritschel, T.: Learning a neural 3d texture space from 2d</div><div class="paragraph"><span class="p-meta">#216</span>exemplars. In: CVPR (2020)</div><div class="paragraph"><span class="p-meta">#217</span>13. Henzler, P., Rasche, V., Ropinski, T., Ritschel, T.: Single-image tomography: 3d</div><div class="paragraph"><span class="p-meta">#218</span>volumes from 2d cranial x-rays. In: Eurographics (2018)</div><div class="paragraph"><span class="p-meta">#219</span>14. Hornik, K., Stinchcombe, M., White, H.: Multilayer feedforward networks are uni-</div><div class="paragraph"><span class="p-meta">#220</span>versal approximators. Neural Networks (1989)</div><div class="paragraph"><span class="p-meta">#221</span>15. Jiang, C., Sud, A., Makadia, A., Huang, J., Nie√üner, M., Funkhouser, T.: Local</div><div class="paragraph"><span class="p-meta">#222</span>implicit grid representations for 3d scenes. In: CVPR (2020)</div><div class="paragraph"><span class="p-meta">#223</span>16. Kajiya, J.T., Herzen, B.P.V.: Ray tracing volume densities. Computer Graphics</div><div class="paragraph"><span class="p-meta">#224</span>(SIGGRAPH) (1984)</div><div class="paragraph"><span class="p-meta">#225</span>17. Kar, A., H¬®ane, C., Malik, J.: Learning a multi-view stereo machine. In: NeurIPS</div><div class="paragraph"><span class="p-meta">#226</span>(2017)</div><div class="paragraph"><span class="p-meta">#227</span>18. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR</div><div class="paragraph"><span class="p-meta">#228</span>(2015)</div><div class="paragraph"><span class="p-meta">#229</span>16</div><div class="paragraph"><span class="p-meta">#230</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph"><span class="p-meta">#231</span>19. Kutulakos, K.N., Seitz, S.M.: A theory of shape by space carving. International</div><div class="paragraph"><span class="p-meta">#232</span>Journal of Computer Vision (2000)</div><div class="paragraph"><span class="p-meta">#233</span>20. Levoy, M.: EÔ¨Écient ray tracing of volume data. ACM Transactions on Graphics</div><div class="paragraph"><span class="p-meta">#234</span>(1990)</div><div class="paragraph ai-generated"><span class="p-meta">#235</span>21. Levoy, M., Hanrahan, P.: Light Ô¨Åeld rendering. In: SIGGRAPH (1996) 22. Li, T.M., Aittala, M., Durand, F., Lehtinen, J.: DiÔ¨Äerentiable monte carlo ray tracing through edge sampling. ACM Transactions on Graphics (SIGGRAPH Asia) (2018)</div><div class="paragraph"><span class="p-meta">#236</span>23. Liu, S., Li, T., Chen, W., Li, H.: Soft rasterizer: A diÔ¨Äerentiable renderer for image-</div><div class="paragraph"><span class="p-meta">#237</span>based 3D reasoning. In: ICCV (2019)</div><div class="paragraph"><span class="p-meta">#238</span>24. Lombardi, S., Simon, T., Saragih, J., Schwartz, G., Lehrmann, A., Sheikh, Y.: Neural volumes: Learning dynamic renderable volumes from images. ACM Trans- actions on Graphics (SIGGRAPH) (2019)</div><div class="paragraph"><span class="p-meta">#239</span>25. Loper, M.M., Black, M.J.: OpenDR: An approximate diÔ¨Äerentiable renderer. In:</div><div class="paragraph"><span class="p-meta">#240</span>ECCV (2014)</div><div class="paragraph"><span class="p-meta">#241</span>26. Max, N.: Optical models for direct volume rendering. IEEE Transactions on Visu-</div><div class="paragraph"><span class="p-meta">#242</span>alization and Computer Graphics (1995)</div><div class="paragraph"><span class="p-meta">#243</span>27. Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy</div><div class="paragraph"><span class="p-meta">#244</span>networks: Learning 3D reconstruction in function space. In: CVPR (2019)</div><div class="paragraph ai-generated"><span class="p-meta">#245</span>28. Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A.: Local light Ô¨Åeld fusion: Practical view synthesis with prescrip- tive sampling guidelines. ACM Transactions on Graphics (SIGGRAPH) (2019) 29. Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A.: DiÔ¨Äerentiable volumetric rendering: Learning implicit 3D representations without 3D supervision. In: CVPR (2019)</div><div class="paragraph"><span class="p-meta">#246</span>30. Nimier-David, M., Vicini, D., Zeltner, T., Jakob, W.: Mitsuba 2: A retargetable forward and inverse renderer. ACM Transactions on Graphics (SIGGRAPH Asia) (2019)</div><div class="paragraph"><span class="p-meta">#247</span>31. Oechsle, M., Mescheder, L., Niemeyer, M., Strauss, T., Geiger, A.: Texture Ô¨Åelds:</div><div class="paragraph"><span class="p-meta">#248</span>Learning texture representations in function space. In: ICCV (2019)</div><div class="paragraph"><span class="p-meta">#249</span>32. Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: DeepSDF: Learn- ing continuous signed distance functions for shape representation. In: CVPR (2019) 33. Penner, E., Zhang, L.: Soft 3D reconstruction for view synthesis. ACM Transactions</div><div class="paragraph"><span class="p-meta">#250</span>on Graphics (SIGGRAPH Asia) (2017) 34. Porter, T., DuÔ¨Ä, T.: Compositing digital</div><div class="paragraph"><span class="p-meta">#251</span>GRAPH) (1984)</div><div class="paragraph"><span class="p-meta">#252</span>images. Computer Graphics (SIG-</div><div class="paragraph ai-generated"><span class="p-meta">#253</span>35. Rahaman, N., Baratin, A., Arpit, D., Dr¬®axler, F., Lin, M., Hamprecht, F.A., Ben- gio, Y., Courville, A.C.: On the spectral bias of neural networks. In: ICML (2018) 36. Rainer, G., Ghosh, A., Jakob, W., Weyrich, T.: UniÔ¨Åed neural encoding of BTFs.</div><div class="paragraph"><span class="p-meta">#254</span>Computer Graphics Forum (Eurographics) (2020)</div><div class="paragraph"><span class="p-meta">#255</span>37. Rainer, G., Jakob, W., Ghosh, A., Weyrich, T.: Neural BTF compression and</div><div class="paragraph"><span class="p-meta">#256</span>interpolation. Computer Graphics Forum (Eurographics) (2019)</div><div class="paragraph"><span class="p-meta">#257</span>38. Ren, P., Wang, J., Gong, M., Lin, S., Tong, X., Guo, B.: Global illumination with</div><div class="paragraph"><span class="p-meta">#258</span>radiance regression functions. ACM Transactions on Graphics (2013)</div><div class="paragraph ai-generated"><span class="p-meta">#259</span>39. Sch¬®onberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: CVPR (2016) 40. Seitz, S.M., Dyer, C.R.: Photorealistic scene reconstruction by voxel coloring. In-</div><div class="paragraph"><span class="p-meta">#260</span>ternational Journal of Computer Vision (1999)</div><div class="paragraph"><span class="p-meta">#261</span>41. Sitzmann, V., Thies, J., Heide, F., Nie√üner, M., Wetzstein, G., Zollh¬®ofer, M.: Deep-</div><div class="paragraph"><span class="p-meta">#262</span>voxels: Learning persistent 3D feature embeddings. In: CVPR (2019)</div><div class="paragraph"><span class="p-meta">#263</span>42. Sitzmann, V., Zollhoefer, M., Wetzstein, G.: Scene representation networks: Con- tinuous 3D-structure-aware neural scene representations. In: NeurIPS (2019)</div><div class="paragraph"><span class="p-meta">#264</span>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#265</span>17</div><div class="paragraph"><span class="p-meta">#266</span>43. Srinivasan, P.P., Tucker, R., Barron, J.T., Ramamoorthi, R., Ng, R., Snavely, N.: Pushing the boundaries of view extrapolation with multiplane images. In: CVPR (2019)</div><div class="paragraph"><span class="p-meta">#267</span>44. Stanley, K.O.: Compositional pattern producing networks: A novel abstraction of</div><div class="paragraph"><span class="p-meta">#268</span>development. Genetic programming and evolvable machines (2007)</div><div class="paragraph"><span class="p-meta">#269</span>45. Szeliski, R., Golland, P.: Stereo matching with transparency and matting. In: ICCV</div><div class="paragraph"><span class="p-meta">#270</span>(1998)</div><div class="paragraph"><span class="p-meta">#271</span>46. Tulsiani, S., Zhou, T., Efros, A.A., Malik, J.: Multi-view supervision for single-view</div><div class="paragraph"><span class="p-meta">#272</span>reconstruction via diÔ¨Äerentiable ray consistency. In: CVPR (2017)</div><div class="paragraph"><span class="p-meta">#273</span>47. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,</div><div class="paragraph"><span class="p-meta">#274</span>(cid:32)L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)</div><div class="paragraph"><span class="p-meta">#275</span>48. Waechter, M., Moehrle, N., Goesele, M.: Let there be color! Large-scale texturing</div><div class="paragraph"><span class="p-meta">#276</span>of 3D reconstructions. In: ECCV (2014)</div><div class="paragraph ai-generated"><span class="p-meta">#277</span>49. Wood, D.N., Azuma, D.I., Aldinger, K., Curless, B., Duchamp, T., Salesin, D.H., Stuetzle, W.: Surface light Ô¨Åelds for 3D photography. In: SIGGRAPH (2000) 50. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable</div><div class="paragraph"><span class="p-meta">#278</span>eÔ¨Äectiveness of deep features as a perceptual metric. In: CVPR (2018)</div><div class="paragraph"><span class="p-meta">#279</span>51. Zhong, E.D., Bepler, T., Davis, J.H., Berger, B.: Reconstructing continuous distri-</div><div class="paragraph"><span class="p-meta">#280</span>butions of 3D protein structure from cryo-EM images. In: ICLR (2020)</div><div class="paragraph"><span class="p-meta">#281</span>52. Zhou, T., Tucker, R., Flynn, J., FyÔ¨Äe, G., Snavely, N.: Stereo magniÔ¨Åcation: Learn- ing view synthesis using multiplane images. ACM Transactions on Graphics (SIG- GRAPH) (2018)</div><div class="paragraph"><span class="p-meta">#282</span>A Additional Implementation Details</div><div class="paragraph"><span class="p-meta">#283</span>Network Architecture Fig. 7 details our simple fully-connected architecture.</div><div class="paragraph"><span class="p-meta">#284</span>Volume Bounds Our method renders views by querying the neural radiance Ô¨Åeld representation at continuous 5D coordinates along camera rays. For exper- iments with synthetic images, we scale the scene so that it lies within a cube of side length 2 centered at the origin, and only query the representation within this bounding volume. Our dataset of real images contains content that can ex- ist anywhere between the closest point and inÔ¨Ånity, so we use normalized device coordinates to map the depth range of these points into [‚àí1, 1]. This shifts all the ray origins to the near plane of the scene, maps the perspective rays of the camera to parallel rays in the transformed volume, and uses disparity (inverse depth) instead of metric depth, so all coordinates are now bounded.</div><div class="paragraph"><span class="p-meta">#285</span>Training Details For real scene data, we regularize our network by adding random Gaussian noise with zero mean and unit variance to the output œÉ values (before passing them through the ReLU) during optimization, Ô¨Ånding that this slightly improves visual performance for rendering novel views. We implement our model in TensorÔ¨Çow [1].</div><div class="paragraph"><span class="p-meta">#286</span>Rendering Details To render new views at test time, we sample 64 points per ray through the coarse network and 64 + 128 = 192 points per ray through the Ô¨Åne network, for a total of 256 network queries per ray. Our realistic synthetic</div><div class="paragraph"><span class="p-meta">#287</span>18</div><div class="paragraph"><span class="p-meta">#288</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph ai-generated"><span class="p-meta">#289</span>Fig. 7: A visualization of our fully-connected network architecture. Input vectors are shown in green, intermediate hidden layers are shown in blue, output vectors are shown in red, and the number inside each block signiÔ¨Åes the vector‚Äôs dimen- sion. All layers are standard fully-connected layers, black arrows indicate layers with ReLU activations, orange arrows indicate layers with no activation, dashed black arrows indicate layers with sigmoid activation, and ‚Äú+‚Äù denotes vector concatenation. The positional encoding of the input location (Œ≥(x)) is passed through 8 fully-connected ReLU layers, each with 256 channels. We follow the DeepSDF [32] architecture and include a skip connection that concatenates this input to the Ô¨Åfth layer‚Äôs activation. An additional layer outputs the volume den- sity œÉ (which is rectiÔ¨Åed using a ReLU to ensure that the output volume density is nonnegative) and a 256-dimensional feature vector. This feature vector is con- catenated with the positional encoding of the input viewing direction (Œ≥(d)), and is processed by an additional fully-connected ReLU layer with 128 channels. A Ô¨Ånal layer (with a sigmoid activation) outputs the emitted RGB radiance at position x, as viewed by a ray with direction d.</div><div class="paragraph"><span class="p-meta">#290</span>dataset requires 640k rays per image, and our real scenes require 762k rays per image, resulting in between 150 and 200 million network queries per rendered image. On an NVIDIA V100, this takes approximately 30 seconds per frame.</div><div class="paragraph"><span class="p-meta">#291</span>B Additional Baseline Method Details</div><div class="paragraph ai-generated"><span class="p-meta">#292</span>Neural Volumes (NV) [24] We use the NV code open-sourced by the authors at https://github.com/facebookresearch/neuralvolumes and follow their procedure for training on a single scene without time dependence.</div><div class="paragraph ai-generated"><span class="p-meta">#293</span>Scene Representation Networks (SRN) [42] We use the SRN code open- sourced by the authors at https://github.com/vsitzmann/scene-representation-networks and follow their procedure for training on a single scene.</div><div class="paragraph ai-generated"><span class="p-meta">#294</span>Local Light Field Fusion (LLFF) [28] We use the pretrained LLFF model open-sourced by the authors at https://github.com/Fyusion/LLFF.</div><div class="paragraph ai-generated"><span class="p-meta">#295</span>RGB (x)<latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="C39OhB+IczRcjLNINXH29e9lt8M=">AAAB2HicbZDNSgMxFIXv1L86Vq1rN8EiuCpTN+pOcOOygmML7VAymTttaCYzJHeEMvQFXLhRfDB3vo3pz0KtBwIf5yTk3hMXSloKgi+vtrW9s7tX3/cPGv7h0XGz8WTz0ggMRa5y04+5RSU1hiRJYb8wyLNYYS+e3i3y3jMaK3P9SLMCo4yPtUyl4OSs7qjZCtrBUmwTOmtowVqj5ucwyUWZoSahuLWDTlBQVHFDUiic+8PSYsHFlI9x4FDzDG1ULcecs3PnJCzNjTua2NL9+aLimbWzLHY3M04T+zdbmP9lg5LS66iSuigJtVh9lJaKUc4WO7NEGhSkZg64MNLNysSEGy7INeO7Djp/N96E8LJ90w4eAqjDKZzBBXTgCm7hHroQgoAEXuDNm3iv3vuqqpq37uwEfsn7+Aap5IoM</latexit><latexit sha1_base64="hQhj+CULGdG3QS0LVa1KRUGPisw=">AAAB73icbVA9T8MwFHwpX6UUCF1ZLCqkslQJC7AhsTAWidBKbVQ5rtNatZ3IdlCrKH+FhQEQ/4aNf4PTdoCWkyyd7t7TO1+UcqaN5307la3tnd296n7toH54dOye1J90kilCA5LwRPUirClnkgaGGU57qaJYRJx2o+ld6XefqdIskY9mntJQ4LFkMSPYWGnoNgZjLARuDQQ2kyjOZ8XF0G16bW8BtEn8FWnCCp2h+zUYJSQTVBrCsdZ930tNmGNlGOG0qA0yTVNMpnhM+5ZKLKgO80X2Ap1bZYTiRNknDVqovzdyLLSei8hOlhH1uleK/3n9zMTXYc5kmhkqyfJQnHFkElQWgUZMUWL43BJMFLNZEZlghYmxddVsCf76lzdJcNm+aXsPHlThFM6gBT5cwS3cQwcCIDCDF3iDd6dwXp2PZVsVZ1VbA/7A+fwB4CiSuw==</latexit><latexit sha1_base64="hQhj+CULGdG3QS0LVa1KRUGPisw=">AAAB73icbVA9T8MwFHwpX6UUCF1ZLCqkslQJC7AhsTAWidBKbVQ5rtNatZ3IdlCrKH+FhQEQ/4aNf4PTdoCWkyyd7t7TO1+UcqaN5307la3tnd296n7toH54dOye1J90kilCA5LwRPUirClnkgaGGU57qaJYRJx2o+ld6XefqdIskY9mntJQ4LFkMSPYWGnoNgZjLARuDQQ2kyjOZ8XF0G16bW8BtEn8FWnCCp2h+zUYJSQTVBrCsdZ930tNmGNlGOG0qA0yTVNMpnhM+5ZKLKgO80X2Ap1bZYTiRNknDVqovzdyLLSei8hOlhH1uleK/3n9zMTXYc5kmhkqyfJQnHFkElQWgUZMUWL43BJMFLNZEZlghYmxddVsCf76lzdJcNm+aXsPHlThFM6gBT5cwS3cQwcCIDCDF3iDd6dwXp2PZVsVZ1VbA/7A+fwB4CiSuw==</latexit><latexit sha1_base64="Wrp6sfGRkT1YIiwCAWhsM6HtC1M=">AAAB+nicbVA9T8MwFHzhs5SvUEaWiAqpLFXKAmwVLIxFIrRSE1WO67RWbSeyHdQqyl9hYQDEyi9h49/gtBmg5SRLp7v39M4XJowq7brf1tr6xubWdmWnuru3f3BoH9UeVZxKTDwcs1j2QqQIo4J4mmpGeokkiIeMdMPJbeF3n4hUNBYPepaQgKORoBHFSBtpYNf8EeIcNXyO9DiMsml+PrDrbtOdw1klrZLUoURnYH/5wxinnAiNGVKq33ITHWRIaooZyat+qkiC8ASNSN9QgThRQTbPnjtnRhk6USzNE9qZq783MsSVmvHQTBYR1bJXiP95/VRHV0FGRZJqIvDiUJQyR8dOUYQzpJJgzWaGICypyergMZIIa1NX1ZTQWv7yKvEumtdN996tt2/KNipwAqfQgBZcQhvuoAMeYJjCM7zCm5VbL9a79bEYXbPKnWP4A+vzBz63lBw=</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit> (x)<latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="C39OhB+IczRcjLNINXH29e9lt8M=">AAAB2HicbZDNSgMxFIXv1L86Vq1rN8EiuCpTN+pOcOOygmML7VAymTttaCYzJHeEMvQFXLhRfDB3vo3pz0KtBwIf5yTk3hMXSloKgi+vtrW9s7tX3/cPGv7h0XGz8WTz0ggMRa5y04+5RSU1hiRJYb8wyLNYYS+e3i3y3jMaK3P9SLMCo4yPtUyl4OSs7qjZCtrBUmwTOmtowVqj5ucwyUWZoSahuLWDTlBQVHFDUiic+8PSYsHFlI9x4FDzDG1ULcecs3PnJCzNjTua2NL9+aLimbWzLHY3M04T+zdbmP9lg5LS66iSuigJtVh9lJaKUc4WO7NEGhSkZg64MNLNysSEGy7INeO7Djp/N96E8LJ90w4eAqjDKZzBBXTgCm7hHroQgoAEXuDNm3iv3vuqqpq37uwEfsn7+Aap5IoM</latexit><latexit sha1_base64="hQhj+CULGdG3QS0LVa1KRUGPisw=">AAAB73icbVA9T8MwFHwpX6UUCF1ZLCqkslQJC7AhsTAWidBKbVQ5rtNatZ3IdlCrKH+FhQEQ/4aNf4PTdoCWkyyd7t7TO1+UcqaN5307la3tnd296n7toH54dOye1J90kilCA5LwRPUirClnkgaGGU57qaJYRJx2o+ld6XefqdIskY9mntJQ4LFkMSPYWGnoNgZjLARuDQQ2kyjOZ8XF0G16bW8BtEn8FWnCCp2h+zUYJSQTVBrCsdZ930tNmGNlGOG0qA0yTVNMpnhM+5ZKLKgO80X2Ap1bZYTiRNknDVqovzdyLLSei8hOlhH1uleK/3n9zMTXYc5kmhkqyfJQnHFkElQWgUZMUWL43BJMFLNZEZlghYmxddVsCf76lzdJcNm+aXsPHlThFM6gBT5cwS3cQwcCIDCDF3iDd6dwXp2PZVsVZ1VbA/7A+fwB4CiSuw==</latexit><latexit sha1_base64="hQhj+CULGdG3QS0LVa1KRUGPisw=">AAAB73icbVA9T8MwFHwpX6UUCF1ZLCqkslQJC7AhsTAWidBKbVQ5rtNatZ3IdlCrKH+FhQEQ/4aNf4PTdoCWkyyd7t7TO1+UcqaN5307la3tnd296n7toH54dOye1J90kilCA5LwRPUirClnkgaGGU57qaJYRJx2o+ld6XefqdIskY9mntJQ4LFkMSPYWGnoNgZjLARuDQQ2kyjOZ8XF0G16bW8BtEn8FWnCCp2h+zUYJSQTVBrCsdZ930tNmGNlGOG0qA0yTVNMpnhM+5ZKLKgO80X2Ap1bZYTiRNknDVqovzdyLLSei8hOlhH1uleK/3n9zMTXYc5kmhkqyfJQnHFkElQWgUZMUWL43BJMFLNZEZlghYmxddVsCf76lzdJcNm+aXsPHlThFM6gBT5cwS3cQwcCIDCDF3iDd6dwXp2PZVsVZ1VbA/7A+fwB4CiSuw==</latexit><latexit sha1_base64="Wrp6sfGRkT1YIiwCAWhsM6HtC1M=">AAAB+nicbVA9T8MwFHzhs5SvUEaWiAqpLFXKAmwVLIxFIrRSE1WO67RWbSeyHdQqyl9hYQDEyi9h49/gtBmg5SRLp7v39M4XJowq7brf1tr6xubWdmWnuru3f3BoH9UeVZxKTDwcs1j2QqQIo4J4mmpGeokkiIeMdMPJbeF3n4hUNBYPepaQgKORoBHFSBtpYNf8EeIcNXyO9DiMsml+PrDrbtOdw1klrZLUoURnYH/5wxinnAiNGVKq33ITHWRIaooZyat+qkiC8ASNSN9QgThRQTbPnjtnRhk6USzNE9qZq783MsSVmvHQTBYR1bJXiP95/VRHV0FGRZJqIvDiUJQyR8dOUYQzpJJgzWaGICypyergMZIIa1NX1ZTQWv7yKvEumtdN996tt2/KNipwAqfQgBZcQhvuoAMeYJjCM7zCm5VbL9a79bEYXbPKnWP4A+vzBz63lBw=</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit><latexit sha1_base64="6F05rQ6IIUhALFsWYUmKYX8h5zw=">AAAB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfBB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEBByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHHCVuCVpgBKdgf3lD2OcciI0ZkipvuskOsiQ1BQzktf8VJEE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OVV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit> (d)<latexit sha1_base64="Z+u0Yue3pS9TMvqDyVRY2E+Un5Q=">AAAB+nicbVBPS8MwHE39O+e/Oo9egkOYl9GKoN6GXjxOsG6wlpGm6RaWpCVJxVH6Vbx4UPHqJ/HmtzHdetDNB4HHe78fv5cXpowq7Tjf1srq2vrGZm2rvr2zu7dvHzQeVJJJTDycsET2Q6QIo4J4mmpG+qkkiIeM9MLJTen3HolUNBH3epqSgKORoDHFSBtpaDf8EeIctXyO9DiM86g4HdpNp+3MAJeJW5EmqNAd2l9+lOCME6ExQ0oNXCfVQY6kppiRou5niqQIT9CIDAwViBMV5LPsBTwxSgTjRJonNJypvzdyxJWa8tBMlhHVoleK/3mDTMeXQU5Fmmki8PxQnDGoE1gWASMqCdZsagjCkpqsEI+RRFibuuqmBHfxy8vEO2tftZ2782bnumqjBo7AMWgBF1yADrgFXeABDJ7AM3gFb1ZhvVjv1sd8dMWqdg7BH1ifPyGTlAw=</latexit><latexit sha1_base64="Z+u0Yue3pS9TMvqDyVRY2E+Un5Q=">AAAB+nicbVBPS8MwHE39O+e/Oo9egkOYl9GKoN6GXjxOsG6wlpGm6RaWpCVJxVH6Vbx4UPHqJ/HmtzHdetDNB4HHe78fv5cXpowq7Tjf1srq2vrGZm2rvr2zu7dvHzQeVJJJTDycsET2Q6QIo4J4mmpG+qkkiIeM9MLJTen3HolUNBH3epqSgKORoDHFSBtpaDf8EeIctXyO9DiM86g4HdpNp+3MAJeJW5EmqNAd2l9+lOCME6ExQ0oNXCfVQY6kppiRou5niqQIT9CIDAwViBMV5LPsBTwxSgTjRJonNJypvzdyxJWa8tBMlhHVoleK/3mDTMeXQU5Fmmki8PxQnDGoE1gWASMqCdZsagjCkpqsEI+RRFibuuqmBHfxy8vEO2tftZ2782bnumqjBo7AMWgBF1yADrgFXeABDJ7AM3gFb1ZhvVjv1sd8dMWqdg7BH1ifPyGTlAw=</latexit><latexit sha1_base64="Z+u0Yue3pS9TMvqDyVRY2E+Un5Q=">AAAB+nicbVBPS8MwHE39O+e/Oo9egkOYl9GKoN6GXjxOsG6wlpGm6RaWpCVJxVH6Vbx4UPHqJ/HmtzHdetDNB4HHe78fv5cXpowq7Tjf1srq2vrGZm2rvr2zu7dvHzQeVJJJTDycsET2Q6QIo4J4mmpG+qkkiIeM9MLJTen3HolUNBH3epqSgKORoDHFSBtpaDf8EeIctXyO9DiM86g4HdpNp+3MAJeJW5EmqNAd2l9+lOCME6ExQ0oNXCfVQY6kppiRou5niqQIT9CIDAwViBMV5LPsBTwxSgTjRJonNJypvzdyxJWa8tBMlhHVoleK/3mDTMeXQU5Fmmki8PxQnDGoE1gWASMqCdZsagjCkpqsEI+RRFibuuqmBHfxy8vEO2tftZ2782bnumqjBo7AMWgBF1yADrgFXeABDJ7AM3gFb1ZhvVjv1sd8dMWqdg7BH1ifPyGTlAw=</latexit> <latexit sha1_base64="PHtNjW6na207435B/B6JIWe5ANM=">AAAB7HicbVDLSgNBEOz1GeMr6tHLYBA8hV0R1FvQi8cIbhJIljA7mU3GzGOZmRXCkn/w4kHFqx/kzb9xkuxBEwsaiqpuurvilDNjff/bW1ldW9/YLG2Vt3d29/YrB4dNozJNaEgUV7odY0M5kzS0zHLaTjXFIua0FY9up37riWrDlHyw45RGAg8kSxjB1knNrmEDgXuVql/zZ0DLJChIFQo0epWvbl+RTFBpCcfGdAI/tVGOtWWE00m5mxmaYjLCA9pxVGJBTZTPrp2gU6f0UaK0K2nRTP09kWNhzFjErlNgOzSL3lT8z+tkNrmKcibTzFJJ5ouSjCOr0PR11GeaEsvHjmCimbsVkSHWmFgXUNmFECy+vEzC89p1zb+/qNZvijRKcAwncAYBXEId7qABIRB4hGd4hTdPeS/eu/cxb13xipkj+APv8wcIeY72</latexit><latexit sha1_base64="PHtNjW6na207435B/B6JIWe5ANM=">AAAB7HicbVDLSgNBEOz1GeMr6tHLYBA8hV0R1FvQi8cIbhJIljA7mU3GzGOZmRXCkn/w4kHFqx/kzb9xkuxBEwsaiqpuurvilDNjff/bW1ldW9/YLG2Vt3d29/YrB4dNozJNaEgUV7odY0M5kzS0zHLaTjXFIua0FY9up37riWrDlHyw45RGAg8kSxjB1knNrmEDgXuVql/zZ0DLJChIFQo0epWvbl+RTFBpCcfGdAI/tVGOtWWE00m5mxmaYjLCA9pxVGJBTZTPrp2gU6f0UaK0K2nRTP09kWNhzFjErlNgOzSL3lT8z+tkNrmKcibTzFJJ5ouSjCOr0PR11GeaEsvHjmCimbsVkSHWmFgXUNmFECy+vEzC89p1zb+/qNZvijRKcAwncAYBXEId7qABIRB4hGd4hTdPeS/eu/cxb13xipkj+APv8wcIeY72</latexit><latexit sha1_base64="PHtNjW6na207435B/B6JIWe5ANM=">AAAB7HicbVDLSgNBEOz1GeMr6tHLYBA8hV0R1FvQi8cIbhJIljA7mU3GzGOZmRXCkn/w4kHFqx/kzb9xkuxBEwsaiqpuurvilDNjff/bW1ldW9/YLG2Vt3d29/YrB4dNozJNaEgUV7odY0M5kzS0zHLaTjXFIua0FY9up37riWrDlHyw45RGAg8kSxjB1knNrmEDgXuVql/zZ0DLJChIFQo0epWvbl+RTFBpCcfGdAI/tVGOtWWE00m5mxmaYjLCA9pxVGJBTZTPrp2gU6f0UaK0K2nRTP09kWNhzFjErlNgOzSL3lT8z+tkNrmKcibTzFJJ5ouSjCOr0PR11GeaEsvHjmCimbsVkSHWmFgXUNmFECy+vEzC89p1zb+/qNZvijRKcAwncAYBXEId7qABIRB4hGd4hTdPeS/eu/cxb13xipkj+APv8wcIeY72</latexit>++602562562562562562562562566024256128NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#296</span>19</div><div class="paragraph ai-generated"><span class="p-meta">#297</span>Quantitative Comparisons The SRN implementation published by the au- thors requires a signiÔ¨Åcant amount of GPU memory, and is limited to an image resolution of 512 √ó 512 pixels even when parallelized across 4 NVIDIA V100 GPUs. We compute quantitative metrics for SRN at 512 √ó 512 pixels for our synthetic datasets and 504 √ó 376 pixels for the real datasets, in comparison to 800 √ó 800 and 1008 √ó 752 respectively for the other methods that can be run at higher resolutions.</div><div class="paragraph"><span class="p-meta">#298</span>C NDC ray space derivation</div><div class="paragraph"><span class="p-meta">#299</span>We reconstruct real scenes with ‚Äúforward facing‚Äù captures in the normalized device coordinate (NDC) space that is commonly used as part of the triangle rasterization pipeline. This space is convenient because it preserves parallel lines while converting the z axis (camera axis) to be linear in disparity.</div><div class="paragraph"><span class="p-meta">#300</span>Here we derive the transformation which is applied to rays to map them from camera space to NDC space. The standard 3D perspective projection matrix for homogeneous coordinates is:</div><div class="paragraph"><span class="p-meta">#301</span>M =</div><div class="paragraph"><span class="p-meta">#302</span>Ô£´</div><div class="paragraph"><span class="p-meta">#303</span>Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#304</span>0 0</div><div class="paragraph"><span class="p-meta">#305</span>n r 0 0 n t 0 0 ‚àí(f +n) f ‚àín 0 0 ‚àí1</div><div class="paragraph"><span class="p-meta">#306</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#307</span>Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#308</span>0 0 ‚àí2f n f ‚àín 0</div><div class="paragraph"><span class="p-meta">#309</span>(7)</div><div class="paragraph ai-generated"><span class="p-meta">#310</span>where n, f are the near and far clipping planes and r and t are the right and top bounds of the scene at the near clipping plane. (Note that this is in the convention where the camera is looking in the ‚àíz direction.) To project a homogeneous point (x, y, z, 1)(cid:62), we left-multiply by M and then divide by the fourth coordinate:</div><div class="paragraph"><span class="p-meta">#311</span>Ô£´</div><div class="paragraph"><span class="p-meta">#312</span>Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#313</span>0 0</div><div class="paragraph"><span class="p-meta">#314</span>n r 0 0 n t 0 0 ‚àí(f +n) f ‚àín 0 0 ‚àí1</div><div class="paragraph"><span class="p-meta">#315</span>0 0 ‚àí2f n f ‚àín 0</div><div class="paragraph"><span class="p-meta">#316</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#317</span>Ô£´</div><div class="paragraph"><span class="p-meta">#318</span>Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#319</span>Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#320</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#321</span>Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#322</span>x y z 1</div><div class="paragraph"><span class="p-meta">#323</span>Ô£´</div><div class="paragraph"><span class="p-meta">#324</span>Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#325</span>=</div><div class="paragraph"><span class="p-meta">#326</span>project ‚Üí</div><div class="paragraph"><span class="p-meta">#327</span>Ô£´</div><div class="paragraph"><span class="p-meta">#328</span>Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#329</span>‚àí(f +n)</div><div class="paragraph"><span class="p-meta">#330</span>n r x n t y f ‚àín z ‚àí ‚àí2f n ‚àíz n x ‚àíz r y n ‚àíz t</div><div class="paragraph"><span class="p-meta">#331</span>f ‚àín</div><div class="paragraph"><span class="p-meta">#332</span>(f +n)</div><div class="paragraph"><span class="p-meta">#333</span>f ‚àín ‚àí 2f n</div><div class="paragraph"><span class="p-meta">#334</span>f ‚àín</div><div class="paragraph"><span class="p-meta">#335</span>1 ‚àíz</div><div class="paragraph"><span class="p-meta">#336</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#337</span>Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#338</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#339</span>Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#340</span>(8)</div><div class="paragraph"><span class="p-meta">#341</span>(9)</div><div class="paragraph"><span class="p-meta">#342</span>The projected point is now in normalized device coordinate (NDC) space, where the original viewing frustum has been mapped to the cube [‚àí1, 1]3.</div><div class="paragraph ai-generated"><span class="p-meta">#343</span>Our goal is to take a ray o + td and calculate a ray origin o(cid:48) and direction d(cid:48) in NDC space such that for every t, there exists a new t(cid:48) for which œÄ(o + td) = o(cid:48) + t(cid:48)d(cid:48) (where œÄ is projection using the above matrix). In other words, the projection of the original ray and the NDC space ray trace out the same points (but not necessarily at the same rate).</div><div class="paragraph"><span class="p-meta">#344</span>20</div><div class="paragraph"><span class="p-meta">#345</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph"><span class="p-meta">#346</span>Let us rewrite the projected point from Eqn. 9 as (axx/z, ayy/z, az + bz/z)(cid:62).</div><div class="paragraph"><span class="p-meta">#347</span>The components of the new origin o(cid:48) and direction d(cid:48) must satisfy:</div><div class="paragraph"><span class="p-meta">#348</span>Ô£´</div><div class="paragraph"><span class="p-meta">#349</span>Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#350</span>ax</div><div class="paragraph"><span class="p-meta">#351</span>ox+tdx oz+tdz oy+tdy ay oz+tdz az + bz</div><div class="paragraph"><span class="p-meta">#352</span>oz+tdz</div><div class="paragraph"><span class="p-meta">#353</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#354</span>Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#355</span>=</div><div class="paragraph"><span class="p-meta">#356</span>Ô£´</div><div class="paragraph"><span class="p-meta">#357</span>Ô£≠</div><div class="paragraph"><span class="p-meta">#358</span>o(cid:48) x + t(cid:48)d(cid:48) x o(cid:48) y + t(cid:48)d(cid:48) y z + t(cid:48)d(cid:48) o(cid:48) z</div><div class="paragraph"><span class="p-meta">#359</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#360</span>Ô£∏ .</div><div class="paragraph"><span class="p-meta">#361</span>(10)</div><div class="paragraph"><span class="p-meta">#362</span>To eliminate a degree of freedom, we decide that t(cid:48) = 0 and t = 0 should map to the same point. Substituting t = 0 and t(cid:48) = 0 Eqn. 10 directly gives our NDC space origin o(cid:48):</div><div class="paragraph"><span class="p-meta">#363</span>o(cid:48) =</div><div class="paragraph"><span class="p-meta">#364</span>Ô£´</div><div class="paragraph"><span class="p-meta">#365</span>Ô£≠</div><div class="paragraph"><span class="p-meta">#366</span>o(cid:48) x o(cid:48) y o(cid:48) z</div><div class="paragraph"><span class="p-meta">#367</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#368</span>Ô£∏ =</div><div class="paragraph"><span class="p-meta">#369</span>Ô£´</div><div class="paragraph"><span class="p-meta">#370</span>Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#371</span>ax</div><div class="paragraph"><span class="p-meta">#372</span>ox oz oy ay oz az + bz oz</div><div class="paragraph"><span class="p-meta">#373</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#374</span>Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#375</span>= œÄ(o) .</div><div class="paragraph"><span class="p-meta">#376</span>(11)</div><div class="paragraph"><span class="p-meta">#377</span>This is exactly the projection œÄ(o) of the original ray‚Äôs origin. By substituting this back into Eqn. 10 for arbitrary t, we can determine the values of t(cid:48) and d(cid:48):</div><div class="paragraph"><span class="p-meta">#378</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#379</span>Ô£∏ =</div><div class="paragraph"><span class="p-meta">#380</span>Ô£´</div><div class="paragraph"><span class="p-meta">#381</span>Ô£≠</div><div class="paragraph"><span class="p-meta">#382</span>t(cid:48)d(cid:48) x t(cid:48)d(cid:48) y t(cid:48)d(cid:48) z</div><div class="paragraph"><span class="p-meta">#383</span>Ô£´</div><div class="paragraph"><span class="p-meta">#384</span>Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#385</span>ax</div><div class="paragraph"><span class="p-meta">#386</span>ox+tdx oz+tdz oy+tdy oz+tdz</div><div class="paragraph"><span class="p-meta">#387</span>ay az + bz</div><div class="paragraph"><span class="p-meta">#388</span>oz+tdz</div><div class="paragraph"><span class="p-meta">#389</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#390</span>Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#391</span>‚àí ax</div><div class="paragraph"><span class="p-meta">#392</span>ox oz oy ‚àí ay oz ‚àí az ‚àí bz oz</div><div class="paragraph"><span class="p-meta">#393</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#394</span>Ô£∑ Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#395</span>ax</div><div class="paragraph"><span class="p-meta">#396</span>ay</div><div class="paragraph"><span class="p-meta">#397</span>Ô£´</div><div class="paragraph"><span class="p-meta">#398</span>Ô£¨ Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#399</span>Ô£´</div><div class="paragraph"><span class="p-meta">#400</span>Ô£¨ Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#401</span>ax</div><div class="paragraph"><span class="p-meta">#402</span>ay</div><div class="paragraph"><span class="p-meta">#403</span>=</div><div class="paragraph"><span class="p-meta">#404</span>=</div><div class="paragraph"><span class="p-meta">#405</span>bz</div><div class="paragraph ai-generated"><span class="p-meta">#406</span>oz(ox+tdx)‚àíox(oz+tdz) (oz+tdz)oz oz(oy+tdy)‚àíoy(oz+tdz) (oz+tdz)oz oz‚àí(oz+tdz) (oz+tdz)oz (cid:16) dx dz (cid:16) dy dz tdz oz+tdz</div><div class="paragraph"><span class="p-meta">#407</span>tdz oz+tdz ‚àíbz</div><div class="paragraph"><span class="p-meta">#408</span>‚àí ox oz ‚àí oy oz</div><div class="paragraph"><span class="p-meta">#409</span>tdz oz+tdz</div><div class="paragraph"><span class="p-meta">#410</span>Ô£∑ Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#411</span>1 oz</div><div class="paragraph"><span class="p-meta">#412</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#413</span>(cid:17)</div><div class="paragraph"><span class="p-meta">#414</span>(cid:17)</div><div class="paragraph"><span class="p-meta">#415</span>Factoring out a common expression that depends only on t gives us:</div><div class="paragraph"><span class="p-meta">#416</span>= 1 ‚àí</div><div class="paragraph"><span class="p-meta">#417</span>oz oz + tdz</div><div class="paragraph"><span class="p-meta">#418</span>t(cid:48) =</div><div class="paragraph"><span class="p-meta">#419</span>d(cid:48) =</div><div class="paragraph"><span class="p-meta">#420</span>Ô£´</div><div class="paragraph"><span class="p-meta">#421</span>tdz oz + tdz (cid:16) dx dz (cid:16) dy dz ‚àíbz</div><div class="paragraph"><span class="p-meta">#422</span>Ô£¨ Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#423</span>ax</div><div class="paragraph"><span class="p-meta">#424</span>ay</div><div class="paragraph"><span class="p-meta">#425</span>(cid:17)</div><div class="paragraph"><span class="p-meta">#426</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#427</span>(cid:17)</div><div class="paragraph"><span class="p-meta">#428</span>Ô£∑ Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#429</span>.</div><div class="paragraph"><span class="p-meta">#430</span>‚àí ox oz ‚àí oy oz</div><div class="paragraph"><span class="p-meta">#431</span>1 oz</div><div class="paragraph"><span class="p-meta">#432</span>(12)</div><div class="paragraph"><span class="p-meta">#433</span>(13)</div><div class="paragraph"><span class="p-meta">#434</span>(14)</div><div class="paragraph"><span class="p-meta">#435</span>(15)</div><div class="paragraph"><span class="p-meta">#436</span>(16)</div><div class="paragraph"><span class="p-meta">#437</span>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#438</span>21</div><div class="paragraph"><span class="p-meta">#439</span>Note that, as desired, t(cid:48) = 0 when t = 0. Additionally, we see that t(cid:48) ‚Üí 1 as t ‚Üí ‚àû. Going back to the original projection matrix, our constants are:</div><div class="paragraph"><span class="p-meta">#440</span>ax = ‚àí</div><div class="paragraph"><span class="p-meta">#441</span>ay = ‚àí</div><div class="paragraph"><span class="p-meta">#442</span>az =</div><div class="paragraph"><span class="p-meta">#443</span>bz =</div><div class="paragraph"><span class="p-meta">#444</span>n r n t f + n f ‚àí n 2f n f ‚àí n</div><div class="paragraph"><span class="p-meta">#445</span>Using the standard pinhole camera model, we can reparameterize as:</div><div class="paragraph"><span class="p-meta">#446</span>ax = ‚àí</div><div class="paragraph"><span class="p-meta">#447</span>ay = ‚àí</div><div class="paragraph"><span class="p-meta">#448</span>fcam W/2 fcam H/2</div><div class="paragraph"><span class="p-meta">#449</span>(17)</div><div class="paragraph"><span class="p-meta">#450</span>(18)</div><div class="paragraph"><span class="p-meta">#451</span>(19)</div><div class="paragraph"><span class="p-meta">#452</span>(20)</div><div class="paragraph"><span class="p-meta">#453</span>(21)</div><div class="paragraph"><span class="p-meta">#454</span>(22)</div><div class="paragraph ai-generated"><span class="p-meta">#455</span>where W and H are the width and height of the image in pixels and fcam is the focal length of the camera.</div><div class="paragraph"><span class="p-meta">#456</span>In our real forward facing captures, we assume that the far scene bound is inÔ¨Ånity (this costs us very little since NDC uses the z dimension to represent inverse depth, i.e., disparity). In this limit the z constants simplify to:</div><div class="paragraph"><span class="p-meta">#457</span>az = 1 bz = 2n .</div><div class="paragraph"><span class="p-meta">#458</span>Combining everything together:</div><div class="paragraph"><span class="p-meta">#459</span>o(cid:48) =</div><div class="paragraph"><span class="p-meta">#460</span>d(cid:48) =</div><div class="paragraph"><span class="p-meta">#461</span>Ô£´</div><div class="paragraph"><span class="p-meta">#462</span>Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#463</span>Ô£´</div><div class="paragraph"><span class="p-meta">#464</span>Ô£¨ Ô£¨ Ô£¨ Ô£≠</div><div class="paragraph"><span class="p-meta">#465</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#466</span>Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#467</span>‚àí fcam ox W/2 oz oy ‚àí fcam H/2 oz 1 + 2n oz (cid:16) dx dz (cid:16) dy dz ‚àí2n 1 oz</div><div class="paragraph"><span class="p-meta">#468</span>‚àí fcam W/2</div><div class="paragraph"><span class="p-meta">#469</span>‚àí fcam H/2</div><div class="paragraph"><span class="p-meta">#470</span>‚àí ox oz ‚àí oy oz</div><div class="paragraph"><span class="p-meta">#471</span>(23)</div><div class="paragraph"><span class="p-meta">#472</span>(24)</div><div class="paragraph"><span class="p-meta">#473</span>(25)</div><div class="paragraph"><span class="p-meta">#474</span>(26)</div><div class="paragraph"><span class="p-meta">#475</span>(cid:17)</div><div class="paragraph"><span class="p-meta">#476</span>Ô£∂</div><div class="paragraph"><span class="p-meta">#477</span>(cid:17)</div><div class="paragraph"><span class="p-meta">#478</span>Ô£∑ Ô£∑ Ô£∑ Ô£∏</div><div class="paragraph"><span class="p-meta">#479</span>.</div><div class="paragraph"><span class="p-meta">#480</span>One Ô¨Ånal detail in our implementation: we shift o to the ray‚Äôs intersection with the near plane at z = ‚àín (before this NDC conversion) by taking on = o + tnd for tn = ‚àí(n + oz)/dz. Once we convert to the NDC ray, this allows us to simply sample t(cid:48) linearly from 0 to 1 in order to get a linear sampling in disparity from n to ‚àû in the original space.</div><div class="paragraph"><span class="p-meta">#481</span>22</div><div class="paragraph"><span class="p-meta">#482</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph"><span class="p-meta">#483</span>Pedestal</div><div class="paragraph"><span class="p-meta">#484</span>Cube</div><div class="paragraph"><span class="p-meta">#485</span>Ground Truth NeRF (ours)</div><div class="paragraph"><span class="p-meta">#486</span>LLFF [28]</div><div class="paragraph"><span class="p-meta">#487</span>SRN [42]</div><div class="paragraph"><span class="p-meta">#488</span>NV [24]</div><div class="paragraph"><span class="p-meta">#489</span>Fig. 8: Comparisons on test-set views for scenes from the DeepVoxels [41] syn- thetic dataset. The objects in this dataset have simple geometry and perfectly diÔ¨Äuse reÔ¨Çectance. Because of the large number of input images (479 views) and simplicity of the rendered objects, both our method and LLFF [28] perform nearly perfectly on this data. LLFF still occasionally presents artifacts when in- terpolating between its 3D volumes, as in the top inset for each object. SRN [42] and NV [24] do not have the representational power to render Ô¨Åne details.</div><div class="paragraph"><span class="p-meta">#490</span>D Additional Results</div><div class="paragraph ai-generated"><span class="p-meta">#491</span>Per-scene breakdown Tables 3, 4, 5, and 6 include a breakdown of the quanti- tative results presented in the main paper into per-scene metrics. The per-scene breakdown is consistent with the aggregate quantitative metrics presented in the paper, where our method quantitatively outperforms all baselines. Although LLFF achieves slightly better LPIPS metrics, we urge readers to view our sup- plementary video where our method achieves better multiview consistency and produces fewer artifacts than all baselines.</div><div class="paragraph"><span class="p-meta">#492</span>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#493</span>23</div><div class="paragraph"><span class="p-meta">#494</span>DeepVoxels [41] SRN [42] NV [24] LLFF [28] Ours</div><div class="paragraph"><span class="p-meta">#495</span>PSNR‚Üë</div><div class="paragraph"><span class="p-meta">#496</span>SSIM‚Üë Chair Pedestal Cube Vase Chair Pedestal Cube Vase Chair Pedestal Cube Vase 33.45 36.67 35.15 36.11 42.65</div><div class="paragraph"><span class="p-meta">#497</span>0.99 27.99 28.42 0.982 31.46 28.74 20.39 0.980 26.48 32.97 0.992 32.58 39.19 37.32 0.991</div><div class="paragraph ai-generated"><span class="p-meta">#498</span>‚àí 0.96 0.97 0.093 0.969 0.944 0.096 0.857 0.916 0.051 0.983 0.983 0.996 0.992 0.047</div><div class="paragraph"><span class="p-meta">#499</span>‚àí ‚àí 0.044 0.074 0.117 0.113 0.039 0.064 0.006 0.017</div><div class="paragraph"><span class="p-meta">#500</span>0.97 0.957 0.963 0.983 0.986</div><div class="paragraph"><span class="p-meta">#501</span>32.35 35.91 36.47 35.87 41.44</div><div class="paragraph"><span class="p-meta">#502</span>‚àí 0.081 0.069 0.039 0.024</div><div class="paragraph"><span class="p-meta">#503</span>LPIPS‚Üì</div><div class="paragraph"><span class="p-meta">#504</span>Table 3: Per-scene quantitative results from the DeepVoxels [41] dataset. The ‚Äúscenes‚Äù in this dataset are all diÔ¨Äuse objects with simple geometry, rendered from texture-mapped meshes captured by a 3D scanner. The metrics for the DeepVoxels method are taken directly from their paper, which does not report LPIPS and only reports two signiÔ¨Åcant Ô¨Ågures for SSIM.</div><div class="paragraph"><span class="p-meta">#505</span>PSNR‚Üë</div><div class="paragraph"><span class="p-meta">#506</span>SRN [42] NV [24] LLFF [28] Ours</div><div class="paragraph"><span class="p-meta">#507</span>SRN [42] NV [24] LLFF [28] Ours</div><div class="paragraph"><span class="p-meta">#508</span>SRN [42] NV [24] LLFF [28] Ours</div><div class="paragraph"><span class="p-meta">#509</span>Chair Drums Ficus Hotdog Lego Materials Mic Ship 26.85 26.81 26.96 17.18 20.73 20.60 27.78 30.71 28.33 22.58 24.79 23.93 23.22 27.48 31.41 21.79 21.13 28.72 32.91 28.65 36.18 33.00 25.01 30.13</div><div class="paragraph"><span class="p-meta">#510</span>18.09 24.22 20.72 29.62</div><div class="paragraph"><span class="p-meta">#511</span>20.85 26.08 24.54 32.54</div><div class="paragraph"><span class="p-meta">#512</span>SSIM‚Üë</div><div class="paragraph"><span class="p-meta">#513</span>Ship Chair Drums Ficus Hotdog Lego Materials Mic 0.757 0.849 0.766 0.947 0.923 0.910 0.784 0.946 0.944 0.910 0.873 0.916 0.964 0.965 0.948 0.823 0.896 0.890 0.980 0.856 0.974 0.967 0.925 0.964</div><div class="paragraph"><span class="p-meta">#514</span>0.808 0.888 0.890 0.949</div><div class="paragraph"><span class="p-meta">#515</span>0.809 0.880 0.911 0.961</div><div class="paragraph"><span class="p-meta">#516</span>LPIPS‚Üì</div><div class="paragraph"><span class="p-meta">#517</span>Ship Chair Drums Ficus Hotdog Lego Materials Mic 0.299 0.149 0.267 0.063 0.100 0.106 0.276 0.107 0.109 0.162 0.214 0.109 0.084 0.061 0.064 0.218 0.130 0.126 0.028 0.206 0.121 0.046 0.091 0.044</div><div class="paragraph"><span class="p-meta">#518</span>0.174 0.130 0.117 0.063</div><div class="paragraph"><span class="p-meta">#519</span>0.200 0.175 0.110 0.050</div><div class="paragraph"><span class="p-meta">#520</span>Table 4: Per-scene quantitative results from our realistic synthetic dataset. The ‚Äúscenes‚Äù in this dataset are all objects with more complex gometry and non- Lambertian materials, rendered using Blender‚Äôs Cycles pathtracer.</div><div class="paragraph"><span class="p-meta">#521</span>24</div><div class="paragraph"><span class="p-meta">#522</span>B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</div><div class="paragraph"><span class="p-meta">#523</span>PSNR‚Üë</div><div class="paragraph"><span class="p-meta">#524</span>SRN [42] LLFF [28] Ours</div><div class="paragraph"><span class="p-meta">#525</span>SRN [42] LLFF [28] Ours</div><div class="paragraph"><span class="p-meta">#526</span>Room Fern Leaves Fortress Orchids Flower T-Rex Horns 24.33 18.24 21.37 27.29 22.87 24.63 25.46 28.42 24.70 19.52 22.85 24.15 27.40 26.80 27.45 32.70 25.17 20.92</div><div class="paragraph"><span class="p-meta">#527</span>17.37 18.52 20.36</div><div class="paragraph"><span class="p-meta">#528</span>26.63 29.40 31.16</div><div class="paragraph"><span class="p-meta">#529</span>SSIM‚Üë</div><div class="paragraph"><span class="p-meta">#530</span>Room Fern Leaves Fortress Orchids Flower T-Rex Horns 0.761 0.611 0.883 0.742 0.857 0.840 0.932 0.753 0.880 0.828 0.948 0.792</div><div class="paragraph"><span class="p-meta">#531</span>0.449 0.588 0.641</div><div class="paragraph"><span class="p-meta">#532</span>0.520 0.697 0.690</div><div class="paragraph"><span class="p-meta">#533</span>0.738 0.844 0.827</div><div class="paragraph"><span class="p-meta">#534</span>0.641 0.872 0.881</div><div class="paragraph"><span class="p-meta">#535</span>LPIPS‚Üì</div><div class="paragraph"><span class="p-meta">#536</span>0.440 SRN [42] LLFF [28] 0.155 0.247 0.216 0.316 Ours</div><div class="paragraph"><span class="p-meta">#537</span>Room Fern Leaves Fortress Orchids Flower T-Rex Horns 0.376 0.298 0.288 0.240 0.174 0.222 0.193 0.268 0.249 0.219</div><div class="paragraph"><span class="p-meta">#538</span>0.467 0.313 0.321</div><div class="paragraph"><span class="p-meta">#539</span>0.453 0.173 0.171</div><div class="paragraph"><span class="p-meta">#540</span>0.280</div><div class="paragraph"><span class="p-meta">#541</span>0.459</div><div class="paragraph"><span class="p-meta">#542</span>0.178</div><div class="paragraph"><span class="p-meta">#543</span>Table 5: Per-scene quantitative results from our real image dataset. The scenes in this dataset are all captured with a forward-facing handheld cellphone.</div><div class="paragraph"><span class="p-meta">#544</span>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</div><div class="paragraph"><span class="p-meta">#545</span>25</div><div class="paragraph"><span class="p-meta">#546</span>PSNR‚Üë</div><div class="paragraph"><span class="p-meta">#547</span>1) No PE, VD, H 2) No Pos. Encoding 3) No View Dependence 4) No Hierarchical 5) Far Fewer Images 6) Fewer Images 7) Fewer Frequencies 8) More Frequencies 9) Complete Model</div><div class="paragraph"><span class="p-meta">#548</span>1) No PE, VD, H 2) No Pos. Encoding 3) No View Dependence 4) No Hierarchical 5) Far Fewer Images 6) Fewer Images 7) Fewer Frequencies 8) More Frequencies 9) Complete Model</div><div class="paragraph"><span class="p-meta">#549</span>1) No PE, VD, H 2) No Pos. Encoding 3) No View Dependence 4) No Hierarchical 5) Far Fewer Images 6) Fewer Images 7) Fewer Frequencies 8) More Frequencies 9) Complete Model</div><div class="paragraph"><span class="p-meta">#550</span>Ship Chair Drums Ficus Hotdog Lego Materials Mic 25.12 28.16 32.24 28.44 26.55 30.76 33.16 30.33 25.72 28.62 32.65 30.06 27.73 31.74 35.24 31.32 26.57 30.47 32.77 30.92 27.67 32.33 34.91 32.19 28.26 31.66 36.06 32.19 32.86 35.78 32.87 28.34 32.91 28.65 36.18 33.00</div><div class="paragraph"><span class="p-meta">#551</span>25.17 23.11 29.32 24.54 25.91 23.41 29.25 24.55 24.39 22.62 23.70 27.45 25.29 30.73 29.92 24.65 30.13 25.01</div><div class="paragraph"><span class="p-meta">#552</span>26.38 27.75 29.93 31.42 27.97 31.53 30.77 32.50 32.54</div><div class="paragraph"><span class="p-meta">#553</span>24.69 27.79 24.96 29.22 26.55 28.54 29.77 29.54 29.62</div><div class="paragraph"><span class="p-meta">#554</span>SSIM‚Üë</div><div class="paragraph"><span class="p-meta">#555</span>Ship Chair Drums Ficus Hotdog Lego Materials Mic 0.810 0.955 0.955 0.919 0.824 0.968 0.956 0.938 0.828 0.962 0.961 0.948 0.844 0.973 0.969 0.951 0.832 0.972 0.966 0.956 0.847 0.979 0.971 0.963 0.853 0.973 0.972 0.959 0.980 0.853 0.973 0.967 0.980 0.856 0.974 0.967</div><div class="paragraph"><span class="p-meta">#556</span>0.926 0.896 0.953 0.918 0.938 0.906 0.956 0.914 0.922 0.895 0.911 0.948 0.928 0.965 0.962 0.921 0.964 0.925</div><div class="paragraph"><span class="p-meta">#557</span>0.882 0.903 0.947 0.951 0.930 0.957 0.947 0.961 0.961</div><div class="paragraph"><span class="p-meta">#558</span>0.905 0.933 0.912 0.944 0.925 0.941 0.952 0.948 0.949</div><div class="paragraph"><span class="p-meta">#559</span>LPIPS‚Üì</div><div class="paragraph"><span class="p-meta">#560</span>Ship Chair Drums Ficus Hotdog Lego Materials Mic 0.168 0.261 0.084 0.104 0.095 0.104 0.261 0.041 0.124 0.076 0.148 0.220 0.073 0.112 0.075 0.177 0.249 0.039 0.130 0.065 0.173 0.229 0.035 0.123 0.058 0.166 0.223 0.029 0.121 0.051 0.029 0.087 0.143 0.055 0.219 0.027 0.261 0.116 0.047 0.158 0.028 0.206 0.121 0.046 0.091</div><div class="paragraph"><span class="p-meta">#561</span>0.178 0.128 0.088 0.072 0.081 0.055 0.071 0.050 0.050</div><div class="paragraph"><span class="p-meta">#562</span>0.084 0.050 0.113 0.056 0.082 0.057 0.038 0.045 0.044</div><div class="paragraph"><span class="p-meta">#563</span>0.111 0.079 0.102 0.080 0.079 0.068 0.060 0.064 0.063</div><div class="paragraph ai-generated"><span class="p-meta">#564</span>Table 6: Per-scene quantitative results from our ablation study. The scenes used here are the same as in Table 4.</div>
            </div>
        </div>
    </body>
    </html>
    